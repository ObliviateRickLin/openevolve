{"id": "231a4706-cd36-4877-9e0d-5262f6c416dc", "code": "# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\nimport math\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.module import Module\nfrom torch.nn.parameter import Parameter\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom sklearn.model_selection import train_test_split\n\n\n# --- From hgcn_utils/math_utils.py ---\ndef cosh(x, clamp=15):\n    return x.clamp(-clamp, clamp).cosh()\n\ndef sinh(x, clamp=15):\n    return x.clamp(-clamp, clamp).sinh()\n\ndef tanh(x, clamp=15):\n    return x.clamp(-clamp, clamp).tanh()\n\nclass Artanh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 - input ** 2)\n\nclass Arsinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(1 + z.pow(2))).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 + input ** 2) ** 0.5\n\nclass Arcosh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(min=1.0 + 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(z.pow(2) - 1)).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (input ** 2 - 1) ** 0.5\n\ndef arcosh(x):\n    return Arcosh.apply(x)\n\ndef arsinh(x):\n    return Arsinh.apply(x)\n\ndef artanh(x):\n    return Artanh.apply(x)\n\n\n# --- Helper functions ---\ndef default_device() -> torch.device:\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix.\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo()\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n    )\n    values = torch.Tensor(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\n# --- From manifolds/base.py ---\nclass Manifold(object):\n    def __init__(self):\n        super().__init__()\n        self.eps = 10e-8\n\n    def sqdist(self, p1, p2, c):\n        raise NotImplementedError\n\n    def egrad2rgrad(self, p, dp, c):\n        raise NotImplementedError\n\n    def proj(self, p, c):\n        raise NotImplementedError\n\n    def proj_tan(self, u, p, c):\n        raise NotImplementedError\n\n    def proj_tan0(self, u, c):\n        raise NotImplementedError\n\n    def expmap(self, u, p, c):\n        raise NotImplementedError\n\n    def logmap(self, p1, p2, c):\n        raise NotImplementedError\n\n    def expmap0(self, u, c):\n        raise NotImplementedError\n\n    def logmap0(self, p, c):\n        raise NotImplementedError\n\n    def mobius_add(self, x, y, c, dim=-1):\n        raise NotImplementedError\n\n    def mobius_matvec(self, m, x, c):\n        raise NotImplementedError\n\n    def init_weights(self, w, c, irange=1e-5):\n        raise NotImplementedError\n\n    def inner(self, p, c, u, v=None, keepdim=False):\n        raise NotImplementedError\n\n    def ptransp(self, x, y, u, c):\n        raise NotImplementedError\n\n    def ptransp0(self, x, u, c):\n        raise NotImplementedError\n\nclass ManifoldParameter(Parameter):\n    def __new__(cls, data, requires_grad, manifold, c):\n        return Parameter.__new__(cls, data, requires_grad)\n\n    def __init__(self, data, requires_grad, manifold, c):\n        self.c = c\n        self.manifold = manifold\n\n    def __repr__(self):\n        return '{} Parameter containing:\\n'.format(self.manifold.name) + super(Parameter, self).__repr__()\n\n\n# --- From manifolds/hyperboloid.py ---\nclass Hyperboloid(Manifold):\n    def __init__(self):\n        super(Hyperboloid, self).__init__()\n        self.name = 'Hyperboloid'\n        self.eps = {torch.float32: 1e-7, torch.float64: 1e-15}\n        self.min_norm = 1e-15\n        self.max_norm = 1e6\n\n    def minkowski_dot(self, x, y, keepdim=True):\n        res = torch.sum(x * y, dim=-1) - 2 * x[..., 0] * y[..., 0]\n        if keepdim:\n            res = res.view(res.shape + (1,))\n        return res\n\n    def minkowski_norm(self, u, keepdim=True):\n        dot = self.minkowski_dot(u, u, keepdim=keepdim)\n        return torch.sqrt(torch.clamp(dot, min=self.eps[u.dtype]))\n\n    def sqdist(self, x, y, c):\n        K = 1. / c\n        prod = self.minkowski_dot(x, y)\n        theta = torch.clamp(-prod / K, min=1.0 + self.eps[x.dtype])\n        sqdist = K * arcosh(theta) ** 2\n        return torch.clamp(sqdist, max=50.0)\n\n    def proj(self, x, c):\n        K = 1. / c\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d)\n        y_sqnorm = torch.norm(y, p=2, dim=1, keepdim=True) ** 2\n        mask = torch.ones_like(x)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(x)\n        vals[:, 0:1] = torch.sqrt(torch.clamp(K + y_sqnorm, min=self.eps[x.dtype]))\n        return vals + mask * x\n\n    def proj_tan(self, u, x, c):\n        d = x.size(1) - 1\n        ux = torch.sum(x.narrow(-1, 1, d) * u.narrow(-1, 1, d), dim=1, keepdim=True)\n        mask = torch.ones_like(u)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(u)\n        vals[:, 0:1] = ux / torch.clamp(x[:, 0:1], min=self.eps[x.dtype])\n        return vals + mask * u\n\n    def expmap(self, u, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, max=self.max_norm)\n        theta = normu / sqrtK\n        theta = torch.clamp(theta, min=self.min_norm)\n        result = cosh(theta) * x + sinh(theta) * u / theta\n        return self.proj(result, c)\n\n    def logmap(self, x, y, c):\n        K = 1. / c\n        xy = torch.clamp(self.minkowski_dot(x, y) + K, max=-self.eps[x.dtype]) - K\n        u = y + xy * x * c\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, min=self.min_norm)\n        dist = self.sqdist(x, y, c) ** 0.5\n        result = dist * u / normu\n        return self.proj_tan(result, x, c)\n\n    def expmap0(self, u, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = u.size(-1) - 1\n        x = u.narrow(-1, 1, d).view(-1, d)\n        x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n        x_norm = torch.clamp(x_norm, min=self.min_norm)\n        theta = x_norm / sqrtK\n        res = torch.ones_like(u)\n        res[:, 0:1] = sqrtK * cosh(theta)\n        res[:, 1:] = sqrtK * sinh(theta) * x / x_norm\n        return self.proj(res, c)\n\n    def logmap0(self, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d).view(-1, d)\n        y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n        y_norm = torch.clamp(y_norm, min=self.min_norm)\n        res = torch.zeros_like(x)\n        theta = torch.clamp(x[:, 0:1] / sqrtK, min=1.0 + self.eps[x.dtype])\n        res[:, 1:] = sqrtK * arcosh(theta) * y / y_norm\n        return res\n\n    def ptransp(self, x, y, u, c):\n        logxy = self.logmap(x, y, c)\n        logyx = self.logmap(y, x, c)\n        sqdist = torch.clamp(self.sqdist(x, y, c), min=self.min_norm)\n        alpha = self.minkowski_dot(logxy, u) / sqdist\n        res = u - alpha * (logxy + logyx)\n        return self.proj_tan(res, y, c)\n\n    def egrad2rgrad(self, x, grad, k, dim=-1):\n        grad.narrow(-1, 0, 1).mul_(-1)\n        grad = grad.addcmul(self.inner(x, grad, dim=dim, keepdim=True), x / k)\n        return grad\n\n    def inner(self, u, v, keepdim: bool = False, dim: int = -1):\n        d = u.size(dim) - 1\n        uv = u * v\n        if keepdim is False:\n            return -uv.narrow(dim, 0, 1).sum(dim=dim, keepdim=False) + uv.narrow(\n                dim, 1, d\n            ).sum(dim=dim, keepdim=False)\n        else:\n            return torch.cat((-uv.narrow(dim, 0, 1), uv.narrow(dim, 1, d)), dim=dim).sum(\n                dim=dim, keepdim=True\n            )\n\n\n# --- RiemannianSGD Optimizer ---\n_default_manifold = Hyperboloid()\n\ndef copy_or_set_(dest, source):\n    if dest.stride() != source.stride():\n        return dest.copy_(source)\n    else:\n        return dest.set_(source)\n\nclass OptimMixin(object):\n    def __init__(self, *args, stabilize=None, **kwargs):\n        self._stabilize = stabilize\n        super().__init__(*args, **kwargs)\n\n    def stabilize_group(self, group):\n        pass\n\n    def stabilize(self):\n        for group in self.param_groups:\n            self.stabilize_group(group)\n\nclass RiemannianSGD(OptimMixin, torch.optim.SGD):\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        with torch.no_grad():\n            for group in self.param_groups:\n                if \"step\" not in group:\n                    group[\"step\"] = 0\n                weight_decay = group[\"weight_decay\"]\n                momentum = group[\"momentum\"]\n                dampening = group[\"dampening\"]\n                nesterov = group[\"nesterov\"]\n                learning_rate = group[\"lr\"]\n\n                for point in group[\"params\"]:\n                    if isinstance(point, ManifoldParameter):\n                        manifold = point.manifold\n                        c = point.c\n                    else:\n                        manifold = _default_manifold\n                        c = 1.\n\n                    grad = point.grad\n                    if grad is None:\n                        continue\n                    if grad.is_sparse:\n                        raise RuntimeError(\"RiemannianSGD does not support sparse gradients\")\n                    \n                    state = self.state[point]\n                    if len(state) == 0:\n                        if momentum > 0:\n                            state[\"momentum_buffer\"] = grad.clone()\n\n                    grad.add_(weight_decay, point)\n                    grad = manifold.egrad2rgrad(point, grad, c)\n\n                    if momentum > 0:\n                        momentum_buffer = state[\"momentum_buffer\"]\n                        momentum_buffer.mul_(momentum).add_(1 - dampening, grad)\n                        if nesterov:\n                            grad = grad.add_(momentum, momentum_buffer)\n                        else:\n                            grad = momentum_buffer\n\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        new_momentum_buffer = manifold.ptransp(point, new_point, momentum_buffer, c)\n                        momentum_buffer.set_(new_momentum_buffer)\n                        copy_or_set_(point, new_point)\n                    else:\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        copy_or_set_(point, new_point)\n\n                    group[\"step\"] += 1\n                if self._stabilize is not None and group[\"step\"] % self._stabilize == 0:\n                    self.stabilize_group(group)\n        return loss\n\n    @torch.no_grad()\n    def stabilize_group(self, group):\n        for p in group[\"params\"]:\n            if not isinstance(p, ManifoldParameter):\n                continue\n            manifold = p.manifold\n            momentum = group[\"momentum\"]\n            copy_or_set_(p, manifold.proj(p))\n            if momentum > 0:\n                param_state = self.state[p]\n                if not param_state:\n                    continue\n                if \"momentum_buffer\" in param_state:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.set_(manifold.proju(p, buf))\n\n\n# --- WarpSampler ---\ndef sample_function(adj_train, num_nodes, batch_size, n_negative, result_queue):\n    num_users, num_items = num_nodes\n    adj_train = lil_matrix(adj_train)\n    all_pairs = np.asarray(adj_train.nonzero()).T\n    user_item_pairs = all_pairs[: adj_train.count_nonzero() // 2]\n    item_user_pairs = all_pairs[adj_train.count_nonzero() // 2:]\n\n    assert len(user_item_pairs) == len(item_user_pairs)\n    np.random.shuffle(user_item_pairs)\n    np.random.shuffle(item_user_pairs)\n\n    all_pairs_set = {idx: set(row) for idx, row in enumerate(adj_train.rows)}\n    user_item_pairs_set = dict(itertools.islice(all_pairs_set.items(), num_users))\n\n    while True:\n        for i in range(int(len(user_item_pairs) / batch_size)):\n            samples_for_users = batch_size\n            user_positive_items_pairs = user_item_pairs[i * samples_for_users: (i + 1) * samples_for_users, :]\n            user_negative_samples = np.random.randint(num_users, sum(num_nodes), size=(samples_for_users, n_negative))\n\n            for user_positive, negatives, i in zip(user_positive_items_pairs,\n                                                   user_negative_samples,\n                                                   range(len(user_negative_samples))):\n                user = user_positive[0]\n                for j, neg in enumerate(negatives):\n                    while neg in user_item_pairs_set[user]:\n                        user_negative_samples[i, j] = neg = np.random.randint(num_users, sum(num_nodes))\n\n            user_triples = np.hstack((user_positive_items_pairs, user_negative_samples))\n            np.random.shuffle(user_triples)\n            result_queue.put(user_triples)\n\nclass WarpSampler(object):\n    def __init__(self, num_nodes, user_item_matrix, batch_size=10000, n_negative=1, n_workers=5):\n        self.result_queue = Queue(maxsize=n_workers * 2)\n        self.processors = []\n        for i in range(n_workers):\n            self.processors.append(\n                Process(target=sample_function, args=(user_item_matrix,\n                                                      num_nodes,\n                                                      batch_size,\n                                                      n_negative,\n                                                      self.result_queue)))\n            self.processors[-1].start()\n\n    def next_batch(self):\n        return self.result_queue.get()\n\n    def close(self):\n        for p in self.processors:\n            p.terminate()\n            p.join()\n\n\n# --- Layers ---\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = t\n\n    def forward(self, dist):\n        # Adjust temperature dynamically\n        dynamic_t = self.t * torch.std(dist) + 1e-4\n        probs = 1. / (torch.exp((dist - self.r) / dynamic_t) + 1)\n        return probs\n\nclass StackGCNs(Module):\n    def __init__(self, num_layers):\n        super(StackGCNs, self).__init__()\n        self.num_gcn_layers = num_layers - 1\n\n    def plainGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return output[-1]\n\n    def resSumGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return sum(output[1:])\n\nclass HypAgg(Module):\n    def __init__(self, manifold, c, in_features, network, num_layers):\n        super(HypAgg, self).__init__()\n        self.manifold = manifold\n        self.c = c\n        self.in_features = in_features\n        self.stackGCNs = getattr(StackGCNs(num_layers), network)\n\n    def forward(self, x, adj):\n        x_tangent = self.manifold.logmap0(x, c=self.c)\n        output = self.stackGCNs((x_tangent, adj))\n        output = output - output.mean(dim=0)\n        output = self.manifold.proj(self.manifold.expmap0(output, c=self.c), c=self.c)\n        return output\n\n    def extra_repr(self):\n        return 'c={}'.format(self.c)\n\nclass HyperbolicGraphConvolution(nn.Module):\n    def __init__(self, manifold, in_features, out_features, c_in, network, num_layers):\n        super(HyperbolicGraphConvolution, self).__init__()\n        self.agg = HypAgg(manifold, c_in, out_features, network, num_layers)\n\n    def forward(self, input):\n        x, adj = input\n        h = self.agg.forward(x, adj)\n        output = h, adj\n        return output\n\nclass Encoder(nn.Module):\n    def __init__(self, c):\n        super(Encoder, self).__init__()\n        self.c = c\n\n    def encode(self, x, adj):\n        if hasattr(self, 'encode_graph') and self.encode_graph:\n            input = (x, adj)\n            output, _ = self.layers.forward(input)\n        else:\n            output = self.layers.forward(x)\n        return output\n\nclass HRCF(Encoder):\n    def __init__(self, c, args):\n        super(HRCF, self).__init__(c)\n        self.manifold = Hyperboloid()\n        assert args.num_layers > 1\n\n        hgc_layers = []\n        in_dim = out_dim = args.embedding_dim\n        hgc_layers.append(\n            HyperbolicGraphConvolution(\n                self.manifold, in_dim, out_dim, self.c, args.network, args.num_layers\n            )\n        )\n        self.layers = nn.Sequential(*hgc_layers)\n        self.encode_graph = True\n\n    def encode(self, x, adj):\n        x_hyp = self.manifold.proj(x, c=self.c)\n        return super(HRCF, self).encode(x_hyp, adj)\n\n\n# --- Data Generator ---\nclass Data(object):\n    def __init__(self, dataset, norm_adj, seed, test_ratio, data_path=\"./data\"):\n        pkl_path = os.path.join(data_path, dataset)\n        self.pkl_path = pkl_path\n        self.dataset = dataset\n        \n        # Load data\n        user_item_list_path = os.path.join(pkl_path, 'user_item_list.pkl')\n        if os.path.exists(user_item_list_path):\n            self.user_item_list = self.load_pickle(user_item_list_path)\n        else:\n            # Create synthetic data if real data not available\n            self.user_item_list = self.create_synthetic_data()\n        \n        self.train_dict, self.test_dict = self.split_data_randomly(self.user_item_list, test_ratio, seed)\n        self.num_users, self.num_items = len(self.user_item_list), max([max(x) for x in self.user_item_list]) + 1\n\n        self.adj_train, user_item = self.generate_adj()\n\n        if eval(norm_adj):\n            self.adj_train_norm = normalize(self.adj_train + sp.eye(self.adj_train.shape[0]))\n            self.adj_train_norm = sparse_mx_to_torch_sparse_tensor(self.adj_train_norm)\n\n        print('num_users %d, num_items %d' % (self.num_users, self.num_items))\n        print('adjacency matrix shape: ', self.adj_train.shape)\n\n        tot_num_rating = sum([len(x) for x in self.user_item_list])\n        print('number of all ratings {}, density {:.6f}'.format(tot_num_rating,\n                                                                tot_num_rating / (self.num_users * self.num_items)))\n\n        self.user_item_csr = self.generate_rating_matrix([*self.train_dict.values()], self.num_users, self.num_items)\n\n    def create_synthetic_data(self):\n        \"\"\"Create synthetic user-item interactions for testing\"\"\"\n        np.random.seed(42)\n        num_users, num_items = 100, 50\n        user_item_list = []\n        \n        for user_id in range(num_users):\n            num_interactions = np.random.randint(5, 20)\n            items = np.random.choice(num_items, num_interactions, replace=False)\n            user_item_list.append(items.tolist())\n        \n        return user_item_list\n\n    def generate_adj(self):\n        user_item = np.zeros((self.num_users, self.num_items)).astype(int)\n        for i, v in self.train_dict.items():\n            user_item[i][v] = 1\n        coo_user_item = sp.coo_matrix(user_item)\n\n        start = time.time()\n        print('generating adj csr... ')\n        rows = np.concatenate((coo_user_item.row, coo_user_item.transpose().row + self.num_users))\n        cols = np.concatenate((coo_user_item.col + self.num_users, coo_user_item.transpose().col))\n        data = np.ones((coo_user_item.nnz * 2,))\n        adj_csr = sp.coo_matrix((data, (rows, cols))).tocsr().astype(np.float32)\n        print('time elapsed: {:.3f}'.format(time.time() - start))\n        return adj_csr, user_item\n\n    def load_pickle(self, name):\n        with open(name, 'rb') as f:\n            return pkl.load(f, encoding='latin1')\n\n    def split_data_randomly(self, user_records, test_ratio, seed):\n        train_dict = {}\n        test_dict = {}\n        for user_id, item_list in enumerate(user_records):\n            tmp_train_sample, tmp_test_sample = train_test_split(item_list, test_size=test_ratio, random_state=seed)\n\n            train_sample = []\n            for place in item_list:\n                if place not in tmp_test_sample:\n                    train_sample.append(place)\n\n            test_sample = []\n            for place in tmp_test_sample:\n                test_sample.append(place)\n\n            train_dict[user_id] = train_sample\n            test_dict[user_id] = test_sample\n        return train_dict, test_dict\n\n    def generate_rating_matrix(self, train_set, num_users, num_items):\n        row = []\n        col = []\n        data = []\n        for user_id, article_list in enumerate(train_set):\n            for article in article_list:\n                row.append(user_id)\n                col.append(article)\n                data.append(1)\n\n        row = np.array(row)\n        col = np.array(col)\n        data = np.array(data)\n        rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n        return rating_matrix\n\n\n# EVOLVE-BLOCK-START\nclass HRCFModel(nn.Module):\n    def __init__(self, users_items, args):\n        super(HRCFModel, self).__init__()\n        self.c = torch.tensor([args.c]).to(default_device())\n        self.manifold = Hyperboloid()\n        self.nnodes = args.n_nodes\n        self.encoder = HRCF(self.c, args)\n        self.num_users, self.num_items = users_items\n        self.margin = args.margin\n        self.weight_decay = args.weight_decay\n        self.num_layers = args.num_layers\n        self.args = args\n        self.dc = FermiDiracDecoder(r=args.r, t=args.t)\n        self.embedding = nn.Embedding(num_embeddings=self.num_users + self.num_items,\n                                      embedding_dim=args.embedding_dim).to(default_device())\n        \n        self.embedding.state_dict()['weight'].uniform_(-args.scale, args.scale)\n        self.embedding.weight = nn.Parameter(self.manifold.expmap0(self.embedding.state_dict()['weight'], self.c))\n        self.embedding.weight = ManifoldParameter(self.embedding.weight, True, self.manifold, self.c)\n        self.alpha = args.alpha\n\n    def encode(self, adj):\n        x = self.embedding.weight\n        if torch.cuda.is_available():\n            adj = adj.to(default_device())\n            x = x.to(default_device())\n        h = self.encoder.encode(x, adj)\n        return h\n\n    def decode(self, h, idx):\n        emb_in = h[idx[:, 0], :]\n        emb_out = h[idx[:, 1], :]\n        sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n        probs = self.dc.forward(sqdist)\n        return [sqdist, probs]\n\n    def geometric_regularizer(self, embeddings):\n        embeddings_tan = self.manifold.logmap0(embeddings, c=1.0)\n        item_embeddings = embeddings_tan[self.num_users:]\n        item_mean_norm = ((1e-6 + item_embeddings.pow(2).sum(dim=1)).mean()).sqrt()\n        return 1.0 / item_mean_norm\n\n    def ranking_loss(self, pos_sqdist, neg_sqdist):\n        loss = pos_sqdist - neg_sqdist + self.margin\n        loss[loss < 0] = 0\n        loss = torch.sum(loss)\n        return loss\n\n    def compute_loss(self, embeddings, triples):\n        train_edges = triples[:, [0, 1]]\n        sampled_false_edges_list = [triples[:, [0, 2 + i]] for i in range(self.args.num_neg)]\n        \n        pos = self.decode(embeddings, train_edges)\n        pos_sqdist, pos_probs = pos\n        neg = self.decode(embeddings, sampled_false_edges_list[0])\n        neg_sqdist, neg_probs = neg\n        \n        ranking_loss = self.ranking_loss(pos_sqdist, neg_sqdist)\n        gr_loss = self.geometric_regularizer(embeddings)\n        \n        return ranking_loss + self.alpha * gr_loss\n\n    def predict(self, h, data):\n        num_users, num_items = data.num_users, data.num_items\n        probs_matrix = np.zeros((num_users, num_items))\n        for i in range(num_users):\n            emb_in = h[i, :]\n            emb_in = emb_in.repeat(num_items).view(num_items, -1)\n            emb_out = h[np.arange(num_users, num_users + num_items), :]\n            sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n            probs = sqdist.detach().cpu().numpy() * -1\n            probs_matrix[i] = np.reshape(probs, [-1, ])\n        return probs_matrix\n# EVOLVE-BLOCK-END\n\n\n# --- Evaluation metrics ---\ndef recall_at_k(test_dict, pred_list, k):\n    recall_list = []\n    for user_id in test_dict:\n        if user_id < len(pred_list):\n            test_items = set(test_dict[user_id])\n            pred_items = set(pred_list[user_id][:k])\n            if len(test_items) > 0:\n                recall = len(test_items & pred_items) / len(test_items)\n                recall_list.append(recall)\n    return np.mean(recall_list) if recall_list else 0.0\n\ndef ndcg_func(ground_truths, ranks):\n    result = 0\n    for i, (rank, ground_truth) in enumerate(zip(ranks, ground_truths)):\n        len_rank = len(rank)\n        len_gt = len(ground_truth)\n        idcg_len = min(len_gt, len_rank)\n\n        idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\n        idcg[idcg_len:] = idcg[idcg_len-1]\n\n        dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\n        result += dcg / idcg\n    return result / len(ranks)\n\ndef eval_rec(pred_matrix, data):\n    topk = 50\n    pred_matrix[data.user_item_csr.nonzero()] = -np.inf\n    ind = np.argpartition(pred_matrix, -topk)\n    ind = ind[:, -topk:]\n    arr_ind = pred_matrix[np.arange(len(pred_matrix))[:, None], ind]\n    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(pred_matrix)), ::-1]\n    pred_list = ind[np.arange(len(pred_matrix))[:, None], arr_ind_argsort]\n    \n    recall = []\n    for k in [5, 10, 20, 50]:\n        recall.append(recall_at_k(data.test_dict, pred_list, k))\n\n    all_ndcg = ndcg_func([*data.test_dict.values()], pred_list)\n    ndcg = [all_ndcg[x-1] for x in [5, 10, 20, 50]]\n\n    return recall, ndcg\n\n\n# --- Main experiment functions ---\ndef run_hrcf_experiment():\n    \"\"\"Main experiment function matching original HRCF exactly\"\"\"\n    try:\n        # Set up arguments exactly like original HRCF Amazon-CD\n        class Args:\n            def __init__(self):\n                # Exact Amazon-CD parameters from run_cd.sh\n                self.embedding_dim = 50\n                self.dim = 50\n                self.num_layers = 8  # Critical: was 4, now 8\n                self.c = 1.0\n                self.margin = 0.15  # Critical: was 0.1, now 0.15\n                self.weight_decay = 5e-3  # Critical: was 0.01, now 5e-3\n                self.alpha = 25  # Critical: was 20, now 25\n                self.r = 2.0\n                self.t = 1.0\n                self.scale = 0.1\n                self.num_neg = 1\n                self.network = 'resSumGCN'\n                self.lr = 0.0015  # Critical: was 0.001, now 0.0015\n                self.momentum = 0.95\n                self.batch_size = 10000  # Critical: was small, now 10000\n                self.epochs = 100  # Increased for better training\n                self.seed = 1234\n                self.test_ratio = 0.2\n                self.norm_adj = 'True'\n        \n        args = Args()\n        \n        # Prepare data exactly like original\n        data = Data(args.dataset if hasattr(args, 'dataset') else 'Amazon-CD', \n                   args.norm_adj, args.seed, args.test_ratio)\n        args.n_nodes = data.num_users + data.num_items\n        args.feat_dim = args.embedding_dim\n        \n        # Initialize model\n        model = HRCFModel((data.num_users, data.num_items), args)\n        model = model.to(default_device())\n        \n        # Use RiemannianSGD exactly like original\n        optimizer = RiemannianSGD(params=model.parameters(), lr=args.lr,\n                                  weight_decay=args.weight_decay, momentum=args.momentum)\n        \n        # Initialize WarpSampler exactly like original\n        sampler = WarpSampler((data.num_users, data.num_items), data.adj_train, \n                             args.batch_size, args.num_neg)\n        \n        num_pairs = data.adj_train.count_nonzero() // 2\n        num_batches = int(num_pairs / args.batch_size) + 1\n        print(f\"Number of batches per epoch: {num_batches}\")\n        \n        # Training loop exactly like original\n        for epoch in range(1, args.epochs + 1):\n            print(f\"--> Starting epoch {epoch}\")\n            avg_loss = 0.\n            t = time.time()\n            \n            for batch in range(num_batches):\n                triples = sampler.next_batch()\n                model.train()\n                optimizer.zero_grad()\n                embeddings = model.encode(data.adj_train_norm)\n                train_loss = model.compute_loss(embeddings, torch.LongTensor(triples))\n                \n                if torch.isnan(train_loss):\n                    print('NaN loss detected!')\n                    break\n                    \n                train_loss.backward()\n                optimizer.step()\n                avg_loss += train_loss / num_batches\n            \n            avg_loss = avg_loss.detach().cpu().numpy()\n            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Time: {time.time() - t:.4f}s\")\n            \n            # Evaluation every 10 epochs\n            if epoch % 10 == 0:\n                model.eval()\n                with torch.no_grad():\n                    embeddings = model.encode(data.adj_train_norm)\n                    pred_matrix = model.predict(embeddings, data)\n                    recall, ndcg = eval_rec(pred_matrix, data)\n                    print(f\"Recall@10: {recall[1]:.4f}, NDCG@10: {ndcg[1]:.4f}\")\n        \n        sampler.close()\n        \n        # Final evaluation\n        model.eval()\n        with torch.no_grad():\n            embeddings = model.encode(data.adj_train_norm)\n            pred_matrix = model.predict(embeddings, data)\n            recall, ndcg = eval_rec(pred_matrix, data)\n            \n        return recall[1]  # Return Recall@10\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment: {e}\")\n        traceback.print_exc()\n        return 0.0\n\n\ndef run_hrcf_experiment_with_embeddings():\n    \"\"\"Extended experiment that returns embeddings\"\"\"\n    try:\n        recall_score = run_hrcf_experiment()\n        \n        # Return dummy embeddings for compatibility\n        num_users, num_items = 100, 50\n        embedding_dim = 50\n        \n        user_embeddings = torch.randn(num_users, embedding_dim) * 0.1\n        item_embeddings = torch.randn(num_items, embedding_dim) * 0.1\n        \n        return {\n            'score': recall_score,\n            'user_embeddings': user_embeddings,\n            'item_embeddings': item_embeddings\n        }\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment_with_embeddings: {e}\")\n    return {\n            'score': 0.0,\n            'user_embeddings': torch.zeros(100, 50),\n            'item_embeddings': torch.zeros(50, 50)\n        }\n\n\nif __name__ == \"__main__\":\n    print(\"Testing HRCF implementation...\")\n    result = run_hrcf_experiment()\n    print(f\"Final Recall@10: {result:.4f}\")\n", "language": "python", "parent_id": "91854f35-362a-4279-b3b6-e2b9d23458a4", "generation": 4, "timestamp": 1751192420.3615239, "iteration_found": 102, "metrics": {"recall_at_10": 0.0906712243347335, "recall_at_20": 0.13727659484909094, "precision_at_10": 0.03357301608053623, "precision_at_20": 0.025966357258030107, "ndcg_at_10": 0.0711432403916877, "ndcg_at_20": 0.08637137321779624, "hit_rate_at_10": 0.24608881335250796, "hit_rate_at_20": 0.33921645531006234, "execution_time": 372.97384691238403, "combined_score": 0.09750278260692424, "error": null}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 19 lines with 15 lines\nChange 2: Replace 4 lines with 5 lines\nChange 3: Replace 5 lines with 6 lines\nChange 4: Replace 6 lines with 6 lines", "parent_metrics": {"recall_at_10": 0.09066875222550702, "recall_at_20": 0.13454050058198602, "precision_at_10": 0.03353379526735804, "precision_at_20": 0.02543687628012462, "ndcg_at_10": 0.07103870943790677, "ndcg_at_20": 0.08525037913411102, "hit_rate_at_10": 0.24608881335250796, "hit_rate_at_20": 0.3349457445417702, "execution_time": 412.5725095272064, "combined_score": 0.09676184473610851, "error": null}, "island": 0}, "artifacts_json": "{\"success_summary\": \"Successful HRCF training with 100 epochs\", \"model_architecture\": \"Embedding dim: 50, Layers: 4, Network: resSumGCN\", \"training_details\": \"Batch size: 10000, LR: 0.001, Weight decay: 0.005\", \"dataset_info\": \"22947 users, 18395 items, 657558 interactions\", \"performance_analysis\": \"Combined score: 0.0975 (Target: >0.10 for good performance)\", \"metric_breakdown\": \"Recall@10: 0.0907, NDCG@10: 0.0711, Precision@10: 0.0336\", \"execution_efficiency\": \"Training time: 373.0s (3.7s per epoch)\", \"recommendation_quality\": \"Excellent\", \"embedding_analysis\": \"Effective rank: 49.0/50 (0.98)\"}", "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert having 10 years experiences in recommendation systems and hyperbolic geometry. Your task is to improve the HRCF (Hyperbolic Regularized Collaborative Filtering) algorithm. Focus on optimizing the loss functions, geometric regularization techniques, and hyperbolic space operations to enhance recommendation quality and convergence speed. Pay special attention to the hyperbolic manifold operations, FermiDirac decoder, and geometric regularization terms. You should make your idea not too complex, leveraging the inherent properties of hyperbolic space to improve performance. You are tirelessly driving the creation of a world-class architecture.", "user": "# Current Program Information\n- Current performance metrics: - recall_at_10: 0.0907\n- recall_at_20: 0.1345\n- precision_at_10: 0.0335\n- precision_at_20: 0.0254\n- ndcg_at_10: 0.0710\n- ndcg_at_20: 0.0853\n- hit_rate_at_10: 0.2461\n- hit_rate_at_20: 0.3349\n- execution_time: 412.5725\n- combined_score: 0.0968\n- error: None\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: recall_at_10, recall_at_20, precision_at_10, precision_at_20, ndcg_at_10, ndcg_at_20, hit_rate_at_10, hit_rate_at_20, combined_score. Consider continuing with similar changes.\n- Metrics showing regression: execution_time. Consider reverting or revising recent changes in these areas.\n\n## Last Execution Output\n\n### success_summary\n```\nSuccessful HRCF training with 100 epochs\n```\n\n### model_architecture\n```\nEmbedding dim: 50, Layers: 4, Network: resSumGCN\n```\n\n### training_details\n```\nBatch size: 10000, LR: 0.001, Weight decay: 0.005\n```\n\n### dataset_info\n```\n22947 users, 18395 items, 657558 interactions\n```\n\n### performance_analysis\n```\nCombined score: 0.0968 (Target: >0.10 for good performance)\n```\n\n### metric_breakdown\n```\nRecall@10: 0.0907, NDCG@10: 0.0710, Precision@10: 0.0335\n```\n\n### execution_efficiency\n```\nTraining time: 412.6s (4.1s per epoch)\n```\n\n### recommendation_quality\n```\nExcellent\n```\n\n### embedding_analysis\n```\nEffective rank: 49.0/50 (0.98)\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0000, recall_at_20: 0.0000, precision_at_10: 0.0000, precision_at_20: 0.0000, ndcg_at_10: 0.0000, ndcg_at_20: 0.0000, hit_rate_at_10: 0.0000, hit_rate_at_20: 0.0000, execution_time: 505.6022, combined_score: 0.0000, error: Training/evaluation error: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.32 GiB is free. Including non-PyTorch memory, this process has 610.00 MiB memory in use. Process 8784 has 470.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 46.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nTraceback (most recent call last):\n  File \"examples/hrcf_recsys/evaluator.py\", line 611, in evaluate\n    pred_matrix = model.predict(embeddings, data)\n  File \"/work/9657011.1.gpu_a100.q/tmpzaktq5rc.py\", line 726, in predict\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.32 GiB is free. Including non-PyTorch memory, this process has 610.00 MiB memory in use. Process 8784 has 470.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 46.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n- Outcome: Mixed results\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0226, recall_at_20: 0.0358, precision_at_10: 0.0084, precision_at_20: 0.0067, ndcg_at_10: 0.0174, ndcg_at_20: 0.0217, hit_rate_at_10: 0.0729, hit_rate_at_20: 0.1095, execution_time: 517.9207, combined_score: 0.0252, error: None\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0218, recall_at_20: 0.0350, precision_at_10: 0.0080, precision_at_20: 0.0066, ndcg_at_10: 0.0167, ndcg_at_20: 0.0210, hit_rate_at_10: 0.0709, hit_rate_at_20: 0.1075, execution_time: 521.2490, combined_score: 0.0244, error: None\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 52.1561)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0218), Performs well on recall_at_20 (0.0350), Performs well on precision_at_10 (0.0080), Performs well on precision_at_20 (0.0066), Performs well on ndcg_at_10 (0.0167), Performs well on ndcg_at_20 (0.0210), Performs well on hit_rate_at_10 (0.0709), Performs well on hit_rate_at_20 (0.1075), Performs well on execution_time (521.2490), Performs well on combined_score (0.0244), Performs well on error (None)\n\n\n### Program 2 (Score: 51.8241)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0226), Performs well on recall_at_20 (0.0358), Performs well on precision_at_10 (0.0084), Performs well on precision_at_20 (0.0067), Performs well on ndcg_at_10 (0.0174), Performs well on ndcg_at_20 (0.0217), Performs well on hit_rate_at_10 (0.0729), Performs well on hit_rate_at_20 (0.1095), Performs well on execution_time (517.9207), Performs well on combined_score (0.0252), Performs well on error (None)\n\n\n### Program 3 (Score: 50.5602)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0000), Performs well on recall_at_20 (0.0000), Performs well on precision_at_10 (0.0000), Performs well on precision_at_20 (0.0000), Performs well on ndcg_at_10 (0.0000), Performs well on ndcg_at_20 (0.0000), Performs well on hit_rate_at_10 (0.0000), Performs well on hit_rate_at_20 (0.0000), Performs well on execution_time (505.6022), Performs well on combined_score (0.0000), Performs well on error (Training/evaluation error: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.32 GiB is free. Including non-PyTorch memory, this process has 610.00 MiB memory in use. Process 8784 has 470.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 46.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nTraceback (most recent call last):\n  File \"examples/hrcf_recsys/evaluator.py\", line 611, in evaluate\n    pred_matrix = model.predict(embeddings, data)\n  File \"/work/9657011.1.gpu_a100.q/tmpzaktq5rc.py\", line 726, in predict\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.32 GiB is free. Including non-PyTorch memory, this process has 610.00 MiB memory in use. Process 8784 has 470.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 46.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n)\n\n\n### Program 4 (Score: 49.7172)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0000), Performs well on recall_at_20 (0.0000), Performs well on precision_at_10 (0.0000), Performs well on precision_at_20 (0.0000), Performs well on ndcg_at_10 (0.0000), Performs well on ndcg_at_20 (0.0000), Performs well on hit_rate_at_10 (0.0000), Performs well on hit_rate_at_20 (0.0000), Performs well on execution_time (497.1724), Performs well on combined_score (0.0000), Performs well on error (Training/evaluation error: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.77 GiB is free. Including non-PyTorch memory, this process has 626.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 62.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nTraceback (most recent call last):\n  File \"examples/hrcf_recsys/evaluator.py\", line 611, in evaluate\n    pred_matrix = model.predict(embeddings, data)\n  File \"/work/9657011.1.gpu_a100.q/tmp6dgi4zd8.py\", line 730, in predict\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.62 GiB. GPU 0 has a total capacity of 39.39 GiB of which 38.77 GiB is free. Including non-PyTorch memory, this process has 626.00 MiB memory in use. Of the allocated memory 55.68 MiB is allocated by PyTorch, and 62.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 49.4490)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\n# ... (truncated)\n```\nKey features: Alternative approach to recall_at_10, Alternative approach to recall_at_20\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 41.1158, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 2 (Score: 52.1561, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 3 (Score: 38.9542, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 4 (Score: 37.5408, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 5 (Score: 41.0023, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n\n# Current Program\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\nimport math\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.module import Module\nfrom torch.nn.parameter import Parameter\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom sklearn.model_selection import train_test_split\n\n\n# --- From hgcn_utils/math_utils.py ---\ndef cosh(x, clamp=15):\n    return x.clamp(-clamp, clamp).cosh()\n\ndef sinh(x, clamp=15):\n    return x.clamp(-clamp, clamp).sinh()\n\ndef tanh(x, clamp=15):\n    return x.clamp(-clamp, clamp).tanh()\n\nclass Artanh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 - input ** 2)\n\nclass Arsinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(1 + z.pow(2))).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 + input ** 2) ** 0.5\n\nclass Arcosh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(min=1.0 + 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(z.pow(2) - 1)).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (input ** 2 - 1) ** 0.5\n\ndef arcosh(x):\n    return Arcosh.apply(x)\n\ndef arsinh(x):\n    return Arsinh.apply(x)\n\ndef artanh(x):\n    return Artanh.apply(x)\n\n\n# --- Helper functions ---\ndef default_device() -> torch.device:\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix.\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo()\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n    )\n    values = torch.Tensor(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\n# --- From manifolds/base.py ---\nclass Manifold(object):\n    def __init__(self):\n        super().__init__()\n        self.eps = 10e-8\n\n    def sqdist(self, p1, p2, c):\n        raise NotImplementedError\n\n    def egrad2rgrad(self, p, dp, c):\n        raise NotImplementedError\n\n    def proj(self, p, c):\n        raise NotImplementedError\n\n    def proj_tan(self, u, p, c):\n        raise NotImplementedError\n\n    def proj_tan0(self, u, c):\n        raise NotImplementedError\n\n    def expmap(self, u, p, c):\n        raise NotImplementedError\n\n    def logmap(self, p1, p2, c):\n        raise NotImplementedError\n\n    def expmap0(self, u, c):\n        raise NotImplementedError\n\n    def logmap0(self, p, c):\n        raise NotImplementedError\n\n    def mobius_add(self, x, y, c, dim=-1):\n        raise NotImplementedError\n\n    def mobius_matvec(self, m, x, c):\n        raise NotImplementedError\n\n    def init_weights(self, w, c, irange=1e-5):\n        raise NotImplementedError\n\n    def inner(self, p, c, u, v=None, keepdim=False):\n        raise NotImplementedError\n\n    def ptransp(self, x, y, u, c):\n        raise NotImplementedError\n\n    def ptransp0(self, x, u, c):\n        raise NotImplementedError\n\nclass ManifoldParameter(Parameter):\n    def __new__(cls, data, requires_grad, manifold, c):\n        return Parameter.__new__(cls, data, requires_grad)\n\n    def __init__(self, data, requires_grad, manifold, c):\n        self.c = c\n        self.manifold = manifold\n\n    def __repr__(self):\n        return '{} Parameter containing:\\n'.format(self.manifold.name) + super(Parameter, self).__repr__()\n\n\n# --- From manifolds/hyperboloid.py ---\nclass Hyperboloid(Manifold):\n    def __init__(self):\n        super(Hyperboloid, self).__init__()\n        self.name = 'Hyperboloid'\n        self.eps = {torch.float32: 1e-7, torch.float64: 1e-15}\n        self.min_norm = 1e-15\n        self.max_norm = 1e6\n\n    def minkowski_dot(self, x, y, keepdim=True):\n        res = torch.sum(x * y, dim=-1) - 2 * x[..., 0] * y[..., 0]\n        if keepdim:\n            res = res.view(res.shape + (1,))\n        return res\n\n    def minkowski_norm(self, u, keepdim=True):\n        dot = self.minkowski_dot(u, u, keepdim=keepdim)\n        return torch.sqrt(torch.clamp(dot, min=self.eps[u.dtype]))\n\n    def sqdist(self, x, y, c):\n        K = 1. / c\n        prod = self.minkowski_dot(x, y)\n        theta = torch.clamp(-prod / K, min=1.0 + self.eps[x.dtype])\n        sqdist = K * arcosh(theta) ** 2\n        return torch.clamp(sqdist, max=50.0)\n\n    def proj(self, x, c):\n        K = 1. / c\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d)\n        y_sqnorm = torch.norm(y, p=2, dim=1, keepdim=True) ** 2\n        mask = torch.ones_like(x)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(x)\n        vals[:, 0:1] = torch.sqrt(torch.clamp(K + y_sqnorm, min=self.eps[x.dtype]))\n        return vals + mask * x\n\n    def proj_tan(self, u, x, c):\n        d = x.size(1) - 1\n        ux = torch.sum(x.narrow(-1, 1, d) * u.narrow(-1, 1, d), dim=1, keepdim=True)\n        mask = torch.ones_like(u)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(u)\n        vals[:, 0:1] = ux / torch.clamp(x[:, 0:1], min=self.eps[x.dtype])\n        return vals + mask * u\n\n    def expmap(self, u, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, max=self.max_norm)\n        theta = normu / sqrtK\n        theta = torch.clamp(theta, min=self.min_norm)\n        result = cosh(theta) * x + sinh(theta) * u / theta\n        return self.proj(result, c)\n\n    def logmap(self, x, y, c):\n        K = 1. / c\n        xy = torch.clamp(self.minkowski_dot(x, y) + K, max=-self.eps[x.dtype]) - K\n        u = y + xy * x * c\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, min=self.min_norm)\n        dist = self.sqdist(x, y, c) ** 0.5\n        result = dist * u / normu\n        return self.proj_tan(result, x, c)\n\n    def expmap0(self, u, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = u.size(-1) - 1\n        x = u.narrow(-1, 1, d).view(-1, d)\n        x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n        x_norm = torch.clamp(x_norm, min=self.min_norm)\n        theta = x_norm / sqrtK\n        res = torch.ones_like(u)\n        res[:, 0:1] = sqrtK * cosh(theta)\n        res[:, 1:] = sqrtK * sinh(theta) * x / x_norm\n        return self.proj(res, c)\n\n    def logmap0(self, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d).view(-1, d)\n        y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n        y_norm = torch.clamp(y_norm, min=self.min_norm)\n        res = torch.zeros_like(x)\n        theta = torch.clamp(x[:, 0:1] / sqrtK, min=1.0 + self.eps[x.dtype])\n        res[:, 1:] = sqrtK * arcosh(theta) * y / y_norm\n        return res\n\n    def ptransp(self, x, y, u, c):\n        logxy = self.logmap(x, y, c)\n        logyx = self.logmap(y, x, c)\n        sqdist = torch.clamp(self.sqdist(x, y, c), min=self.min_norm)\n        alpha = self.minkowski_dot(logxy, u) / sqdist\n        res = u - alpha * (logxy + logyx)\n        return self.proj_tan(res, y, c)\n\n    def egrad2rgrad(self, x, grad, k, dim=-1):\n        grad.narrow(-1, 0, 1).mul_(-1)\n        grad = grad.addcmul(self.inner(x, grad, dim=dim, keepdim=True), x / k)\n        return grad\n\n    def inner(self, u, v, keepdim: bool = False, dim: int = -1):\n        d = u.size(dim) - 1\n        uv = u * v\n        if keepdim is False:\n            return -uv.narrow(dim, 0, 1).sum(dim=dim, keepdim=False) + uv.narrow(\n                dim, 1, d\n            ).sum(dim=dim, keepdim=False)\n        else:\n            return torch.cat((-uv.narrow(dim, 0, 1), uv.narrow(dim, 1, d)), dim=dim).sum(\n                dim=dim, keepdim=True\n            )\n\n\n# --- RiemannianSGD Optimizer ---\n_default_manifold = Hyperboloid()\n\ndef copy_or_set_(dest, source):\n    if dest.stride() != source.stride():\n        return dest.copy_(source)\n    else:\n        return dest.set_(source)\n\nclass OptimMixin(object):\n    def __init__(self, *args, stabilize=None, **kwargs):\n        self._stabilize = stabilize\n        super().__init__(*args, **kwargs)\n\n    def stabilize_group(self, group):\n        pass\n\n    def stabilize(self):\n        for group in self.param_groups:\n            self.stabilize_group(group)\n\nclass RiemannianSGD(OptimMixin, torch.optim.SGD):\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        with torch.no_grad():\n            for group in self.param_groups:\n                if \"step\" not in group:\n                    group[\"step\"] = 0\n                weight_decay = group[\"weight_decay\"]\n                momentum = group[\"momentum\"]\n                dampening = group[\"dampening\"]\n                nesterov = group[\"nesterov\"]\n                learning_rate = group[\"lr\"]\n\n                for point in group[\"params\"]:\n                    if isinstance(point, ManifoldParameter):\n                        manifold = point.manifold\n                        c = point.c\n                    else:\n                        manifold = _default_manifold\n                        c = 1.\n\n                    grad = point.grad\n                    if grad is None:\n                        continue\n                    if grad.is_sparse:\n                        raise RuntimeError(\"RiemannianSGD does not support sparse gradients\")\n                    \n                    state = self.state[point]\n                    if len(state) == 0:\n                        if momentum > 0:\n                            state[\"momentum_buffer\"] = grad.clone()\n\n                    grad.add_(weight_decay, point)\n                    grad = manifold.egrad2rgrad(point, grad, c)\n\n                    if momentum > 0:\n                        momentum_buffer = state[\"momentum_buffer\"]\n                        momentum_buffer.mul_(momentum).add_(1 - dampening, grad)\n                        if nesterov:\n                            grad = grad.add_(momentum, momentum_buffer)\n                        else:\n                            grad = momentum_buffer\n\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        new_momentum_buffer = manifold.ptransp(point, new_point, momentum_buffer, c)\n                        momentum_buffer.set_(new_momentum_buffer)\n                        copy_or_set_(point, new_point)\n                    else:\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        copy_or_set_(point, new_point)\n\n                    group[\"step\"] += 1\n                if self._stabilize is not None and group[\"step\"] % self._stabilize == 0:\n                    self.stabilize_group(group)\n        return loss\n\n    @torch.no_grad()\n    def stabilize_group(self, group):\n        for p in group[\"params\"]:\n            if not isinstance(p, ManifoldParameter):\n                continue\n            manifold = p.manifold\n            momentum = group[\"momentum\"]\n            copy_or_set_(p, manifold.proj(p))\n            if momentum > 0:\n                param_state = self.state[p]\n                if not param_state:\n                    continue\n                if \"momentum_buffer\" in param_state:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.set_(manifold.proju(p, buf))\n\n\n# --- WarpSampler ---\ndef sample_function(adj_train, num_nodes, batch_size, n_negative, result_queue):\n    num_users, num_items = num_nodes\n    adj_train = lil_matrix(adj_train)\n    all_pairs = np.asarray(adj_train.nonzero()).T\n    user_item_pairs = all_pairs[: adj_train.count_nonzero() // 2]\n    item_user_pairs = all_pairs[adj_train.count_nonzero() // 2:]\n\n    assert len(user_item_pairs) == len(item_user_pairs)\n    np.random.shuffle(user_item_pairs)\n    np.random.shuffle(item_user_pairs)\n\n    all_pairs_set = {idx: set(row) for idx, row in enumerate(adj_train.rows)}\n    user_item_pairs_set = dict(itertools.islice(all_pairs_set.items(), num_users))\n\n    while True:\n        for i in range(int(len(user_item_pairs) / batch_size)):\n            samples_for_users = batch_size\n            user_positive_items_pairs = user_item_pairs[i * samples_for_users: (i + 1) * samples_for_users, :]\n            user_negative_samples = np.random.randint(num_users, sum(num_nodes), size=(samples_for_users, n_negative))\n\n            for user_positive, negatives, i in zip(user_positive_items_pairs,\n                                                   user_negative_samples,\n                                                   range(len(user_negative_samples))):\n                user = user_positive[0]\n                for j, neg in enumerate(negatives):\n                    while neg in user_item_pairs_set[user]:\n                        user_negative_samples[i, j] = neg = np.random.randint(num_users, sum(num_nodes))\n\n            user_triples = np.hstack((user_positive_items_pairs, user_negative_samples))\n            np.random.shuffle(user_triples)\n            result_queue.put(user_triples)\n\nclass WarpSampler(object):\n    def __init__(self, num_nodes, user_item_matrix, batch_size=10000, n_negative=1, n_workers=5):\n        self.result_queue = Queue(maxsize=n_workers * 2)\n        self.processors = []\n        for i in range(n_workers):\n            self.processors.append(\n                Process(target=sample_function, args=(user_item_matrix,\n                                                      num_nodes,\n                                                      batch_size,\n                                                      n_negative,\n                                                      self.result_queue)))\n            self.processors[-1].start()\n\n    def next_batch(self):\n        return self.result_queue.get()\n\n    def close(self):\n        for p in self.processors:\n            p.terminate()\n            p.join()\n\n\n# --- Layers ---\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = t\n\n    def forward(self, dist):\n        # Adjust temperature dynamically\n        dynamic_t = self.t * torch.std(dist) + 1e-4\n        probs = 1. / (torch.exp((dist - self.r) / dynamic_t) + 1)\n        return probs\n\nclass StackGCNs(Module):\n    def __init__(self, num_layers):\n        super(StackGCNs, self).__init__()\n        self.num_gcn_layers = num_layers - 1\n\n    def plainGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return output[-1]\n\n    def resSumGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return sum(output[1:])\n\nclass HypAgg(Module):\n    def __init__(self, manifold, c, in_features, network, num_layers):\n        super(HypAgg, self).__init__()\n        self.manifold = manifold\n        self.c = c\n        self.in_features = in_features\n        self.stackGCNs = getattr(StackGCNs(num_layers), network)\n\n    def forward(self, x, adj):\n        x_tangent = self.manifold.logmap0(x, c=self.c)\n        output = self.stackGCNs((x_tangent, adj))\n        output = output - output.mean(dim=0)\n        output = self.manifold.proj(self.manifold.expmap0(output, c=self.c), c=self.c)\n        return output\n\n    def extra_repr(self):\n        return 'c={}'.format(self.c)\n\nclass HyperbolicGraphConvolution(nn.Module):\n    def __init__(self, manifold, in_features, out_features, c_in, network, num_layers):\n        super(HyperbolicGraphConvolution, self).__init__()\n        self.agg = HypAgg(manifold, c_in, out_features, network, num_layers)\n\n    def forward(self, input):\n        x, adj = input\n        h = self.agg.forward(x, adj)\n        output = h, adj\n        return output\n\nclass Encoder(nn.Module):\n    def __init__(self, c):\n        super(Encoder, self).__init__()\n        self.c = c\n\n    def encode(self, x, adj):\n        if hasattr(self, 'encode_graph') and self.encode_graph:\n            input = (x, adj)\n            output, _ = self.layers.forward(input)\n        else:\n            output = self.layers.forward(x)\n        return output\n\nclass HRCF(Encoder):\n    def __init__(self, c, args):\n        super(HRCF, self).__init__(c)\n        self.manifold = Hyperboloid()\n        assert args.num_layers > 1\n\n        hgc_layers = []\n        in_dim = out_dim = args.embedding_dim\n        hgc_layers.append(\n            HyperbolicGraphConvolution(\n                self.manifold, in_dim, out_dim, self.c, args.network, args.num_layers\n            )\n        )\n        self.layers = nn.Sequential(*hgc_layers)\n        self.encode_graph = True\n\n    def encode(self, x, adj):\n        x_hyp = self.manifold.proj(x, c=self.c)\n        return super(HRCF, self).encode(x_hyp, adj)\n\n\n# --- Data Generator ---\nclass Data(object):\n    def __init__(self, dataset, norm_adj, seed, test_ratio, data_path=\"./data\"):\n        pkl_path = os.path.join(data_path, dataset)\n        self.pkl_path = pkl_path\n        self.dataset = dataset\n        \n        # Load data\n        user_item_list_path = os.path.join(pkl_path, 'user_item_list.pkl')\n        if os.path.exists(user_item_list_path):\n            self.user_item_list = self.load_pickle(user_item_list_path)\n        else:\n            # Create synthetic data if real data not available\n            self.user_item_list = self.create_synthetic_data()\n        \n        self.train_dict, self.test_dict = self.split_data_randomly(self.user_item_list, test_ratio, seed)\n        self.num_users, self.num_items = len(self.user_item_list), max([max(x) for x in self.user_item_list]) + 1\n\n        self.adj_train, user_item = self.generate_adj()\n\n        if eval(norm_adj):\n            self.adj_train_norm = normalize(self.adj_train + sp.eye(self.adj_train.shape[0]))\n            self.adj_train_norm = sparse_mx_to_torch_sparse_tensor(self.adj_train_norm)\n\n        print('num_users %d, num_items %d' % (self.num_users, self.num_items))\n        print('adjacency matrix shape: ', self.adj_train.shape)\n\n        tot_num_rating = sum([len(x) for x in self.user_item_list])\n        print('number of all ratings {}, density {:.6f}'.format(tot_num_rating,\n                                                                tot_num_rating / (self.num_users * self.num_items)))\n\n        self.user_item_csr = self.generate_rating_matrix([*self.train_dict.values()], self.num_users, self.num_items)\n\n    def create_synthetic_data(self):\n        \"\"\"Create synthetic user-item interactions for testing\"\"\"\n        np.random.seed(42)\n        num_users, num_items = 100, 50\n        user_item_list = []\n        \n        for user_id in range(num_users):\n            num_interactions = np.random.randint(5, 20)\n            items = np.random.choice(num_items, num_interactions, replace=False)\n            user_item_list.append(items.tolist())\n        \n        return user_item_list\n\n    def generate_adj(self):\n        user_item = np.zeros((self.num_users, self.num_items)).astype(int)\n        for i, v in self.train_dict.items():\n            user_item[i][v] = 1\n        coo_user_item = sp.coo_matrix(user_item)\n\n        start = time.time()\n        print('generating adj csr... ')\n        rows = np.concatenate((coo_user_item.row, coo_user_item.transpose().row + self.num_users))\n        cols = np.concatenate((coo_user_item.col + self.num_users, coo_user_item.transpose().col))\n        data = np.ones((coo_user_item.nnz * 2,))\n        adj_csr = sp.coo_matrix((data, (rows, cols))).tocsr().astype(np.float32)\n        print('time elapsed: {:.3f}'.format(time.time() - start))\n        return adj_csr, user_item\n\n    def load_pickle(self, name):\n        with open(name, 'rb') as f:\n            return pkl.load(f, encoding='latin1')\n\n    def split_data_randomly(self, user_records, test_ratio, seed):\n        train_dict = {}\n        test_dict = {}\n        for user_id, item_list in enumerate(user_records):\n            tmp_train_sample, tmp_test_sample = train_test_split(item_list, test_size=test_ratio, random_state=seed)\n\n            train_sample = []\n            for place in item_list:\n                if place not in tmp_test_sample:\n                    train_sample.append(place)\n\n            test_sample = []\n            for place in tmp_test_sample:\n                test_sample.append(place)\n\n            train_dict[user_id] = train_sample\n            test_dict[user_id] = test_sample\n        return train_dict, test_dict\n\n    def generate_rating_matrix(self, train_set, num_users, num_items):\n        row = []\n        col = []\n        data = []\n        for user_id, article_list in enumerate(train_set):\n            for article in article_list:\n                row.append(user_id)\n                col.append(article)\n                data.append(1)\n\n        row = np.array(row)\n        col = np.array(col)\n        data = np.array(data)\n        rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n        return rating_matrix\n\n\n# EVOLVE-BLOCK-START\nclass HRCFModel(nn.Module):\n    def __init__(self, users_items, args):\n        super(HRCFModel, self).__init__()\n        self.c = torch.tensor([args.c]).to(default_device())\n        self.manifold = Hyperboloid()\n        self.nnodes = args.n_nodes\n        self.encoder = HRCF(self.c, args)\n        self.num_users, self.num_items = users_items\n        self.margin = args.margin\n        self.weight_decay = args.weight_decay\n        self.num_layers = args.num_layers\n        self.args = args\n        self.dc = FermiDiracDecoder(r=args.r, t=args.t)\n        self.embedding = nn.Embedding(num_embeddings=self.num_users + self.num_items,\n                                      embedding_dim=args.embedding_dim).to(default_device())\n        \n        self.embedding.state_dict()['weight'].uniform_(-args.scale, args.scale)\n        self.embedding.weight = nn.Parameter(self.manifold.expmap0(self.embedding.state_dict()['weight'], self.c))\n        self.embedding.weight = ManifoldParameter(self.embedding.weight, True, self.manifold, self.c)\n        self.alpha = args.alpha\n\n    def encode(self, adj):\n        x = self.embedding.weight\n        if torch.cuda.is_available():\n            adj = adj.to(default_device())\n            x = x.to(default_device())\n        h = self.encoder.encode(x, adj)\n        return h\n\n    def decode(self, h, idx):\n        emb_in = h[idx[:, 0], :]\n        emb_out = h[idx[:, 1], :]\n        sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n        probs = self.dc.forward(sqdist)\n        return [sqdist, probs]\n\n    def geometric_regularizer(self, embeddings):\n        embeddings_tan = self.manifold.logmap0(embeddings, c=1.0)\n        item_embeddings = embeddings_tan[self.num_users:]\n        item_mean_norm = ((1e-6 + item_embeddings.pow(2).sum(dim=1)).mean()).sqrt()\n        return 1.0 / item_mean_norm\n\n    def ranking_loss(self, pos_sqdist, neg_sqdist):\n        loss = pos_sqdist - neg_sqdist + self.margin\n        loss[loss < 0] = 0\n        loss = torch.sum(loss)\n        return loss\n\n    def compute_loss(self, embeddings, triples):\n        train_edges = triples[:, [0, 1]]\n        sampled_false_edges_list = [triples[:, [0, 2 + i]] for i in range(self.args.num_neg)]\n        \n        pos = self.decode(embeddings, train_edges)\n        pos_sqdist, pos_probs = pos\n        neg = self.decode(embeddings, sampled_false_edges_list[0])\n        neg_sqdist, neg_probs = neg\n        \n        ranking_loss = self.ranking_loss(pos_sqdist, neg_sqdist)\n        gr_loss = self.geometric_regularizer(embeddings)\n        \n        return ranking_loss + self.alpha * gr_loss\n\n    def predict(self, h, data):\n        num_users, num_items = data.num_users, data.num_items\n        probs_matrix = np.zeros((num_users, num_items))\n        for i in range(num_users):\n            emb_in = h[i, :]\n            emb_in = emb_in.repeat(num_items).view(num_items, -1)\n            emb_out = h[np.arange(num_users, num_users + num_items), :]\n            sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n            probs = sqdist.detach().cpu().numpy() * -1\n            probs_matrix[i] = np.reshape(probs, [-1, ])\n        return probs_matrix\n# EVOLVE-BLOCK-END\n\n\n# --- Evaluation metrics ---\ndef recall_at_k(test_dict, pred_list, k):\n    recall_list = []\n    for user_id in test_dict:\n        if user_id < len(pred_list):\n            test_items = set(test_dict[user_id])\n            pred_items = set(pred_list[user_id][:k])\n            if len(test_items) > 0:\n                recall = len(test_items & pred_items) / len(test_items)\n                recall_list.append(recall)\n    return np.mean(recall_list) if recall_list else 0.0\n\ndef ndcg_func(ground_truths, ranks):\n    result = 0\n    for i, (rank, ground_truth) in enumerate(zip(ranks, ground_truths)):\n        len_rank = len(rank)\n        len_gt = len(ground_truth)\n        idcg_len = min(len_gt, len_rank)\n\n        idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\n        idcg[idcg_len:] = idcg[idcg_len-1]\n\n        dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\n        result += dcg / idcg\n    return result / len(ranks)\n\ndef eval_rec(pred_matrix, data):\n    topk = 50\n    pred_matrix[data.user_item_csr.nonzero()] = -np.inf\n    ind = np.argpartition(pred_matrix, -topk)\n    ind = ind[:, -topk:]\n    arr_ind = pred_matrix[np.arange(len(pred_matrix))[:, None], ind]\n    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(pred_matrix)), ::-1]\n    pred_list = ind[np.arange(len(pred_matrix))[:, None], arr_ind_argsort]\n    \n    recall = []\n    for k in [5, 10, 20, 50]:\n        recall.append(recall_at_k(data.test_dict, pred_list, k))\n\n    all_ndcg = ndcg_func([*data.test_dict.values()], pred_list)\n    ndcg = [all_ndcg[x-1] for x in [5, 10, 20, 50]]\n\n    return recall, ndcg\n\n\n# --- Main experiment functions ---\ndef run_hrcf_experiment():\n    \"\"\"Main experiment function matching original HRCF exactly\"\"\"\n    try:\n        # Set up arguments exactly like original HRCF Amazon-CD\n        class Args:\n            def __init__(self):\n                # Exact Amazon-CD parameters from run_cd.sh\n                self.embedding_dim = 50\n                self.dim = 50\n                self.num_layers = 8  # Critical: was 4, now 8\n                self.c = 1.0\n                self.margin = 0.15  # Critical: was 0.1, now 0.15\n                self.weight_decay = 5e-3  # Critical: was 0.01, now 5e-3\n                self.alpha = 25  # Critical: was 20, now 25\n                self.r = 2.0\n                self.t = 1.0\n                self.scale = 0.1\n                self.num_neg = 1\n                self.network = 'resSumGCN'\n                self.lr = 0.0015  # Critical: was 0.001, now 0.0015\n                self.momentum = 0.95\n                self.batch_size = 10000  # Critical: was small, now 10000\n                self.epochs = 100  # Increased for better training\n                self.seed = 1234\n                self.test_ratio = 0.2\n                self.norm_adj = 'True'\n        \n        args = Args()\n        \n        # Prepare data exactly like original\n        data = Data(args.dataset if hasattr(args, 'dataset') else 'Amazon-CD', \n                   args.norm_adj, args.seed, args.test_ratio)\n        args.n_nodes = data.num_users + data.num_items\n        args.feat_dim = args.embedding_dim\n        \n        # Initialize model\n        model = HRCFModel((data.num_users, data.num_items), args)\n        model = model.to(default_device())\n        \n        # Use RiemannianSGD exactly like original\n        optimizer = RiemannianSGD(params=model.parameters(), lr=args.lr,\n                                  weight_decay=args.weight_decay, momentum=args.momentum)\n        \n        # Initialize WarpSampler exactly like original\n        sampler = WarpSampler((data.num_users, data.num_items), data.adj_train, \n                             args.batch_size, args.num_neg)\n        \n        num_pairs = data.adj_train.count_nonzero() // 2\n        num_batches = int(num_pairs / args.batch_size) + 1\n        print(f\"Number of batches per epoch: {num_batches}\")\n        \n        # Training loop exactly like original\n        for epoch in range(1, args.epochs + 1):\n            print(f\"--> Starting epoch {epoch}\")\n            avg_loss = 0.\n            t = time.time()\n            \n            for batch in range(num_batches):\n                triples = sampler.next_batch()\n                model.train()\n                optimizer.zero_grad()\n                embeddings = model.encode(data.adj_train_norm)\n                train_loss = model.compute_loss(embeddings, torch.LongTensor(triples))\n                \n                if torch.isnan(train_loss):\n                    print('NaN loss detected!')\n                    break\n                    \n                train_loss.backward()\n                optimizer.step()\n                avg_loss += train_loss / num_batches\n            \n            avg_loss = avg_loss.detach().cpu().numpy()\n            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Time: {time.time() - t:.4f}s\")\n            \n            # Evaluation every 10 epochs\n            if epoch % 10 == 0:\n                model.eval()\n                with torch.no_grad():\n                    embeddings = model.encode(data.adj_train_norm)\n                    pred_matrix = model.predict(embeddings, data)\n                    recall, ndcg = eval_rec(pred_matrix, data)\n                    print(f\"Recall@10: {recall[1]:.4f}, NDCG@10: {ndcg[1]:.4f}\")\n        \n        sampler.close()\n        \n        # Final evaluation\n        model.eval()\n        with torch.no_grad():\n            embeddings = model.encode(data.adj_train_norm)\n            pred_matrix = model.predict(embeddings, data)\n            recall, ndcg = eval_rec(pred_matrix, data)\n            \n        return recall[1]  # Return Recall@10\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment: {e}\")\n        traceback.print_exc()\n        return 0.0\n\n\ndef run_hrcf_experiment_with_embeddings():\n    \"\"\"Extended experiment that returns embeddings\"\"\"\n    try:\n        recall_score = run_hrcf_experiment()\n        \n        # Return dummy embeddings for compatibility\n        num_users, num_items = 100, 50\n        embedding_dim = 50\n        \n        user_embeddings = torch.randn(num_users, embedding_dim) * 0.1\n        item_embeddings = torch.randn(num_items, embedding_dim) * 0.1\n        \n        return {\n            'score': recall_score,\n            'user_embeddings': user_embeddings,\n            'item_embeddings': item_embeddings\n        }\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment_with_embeddings: {e}\")\n    return {\n            'score': 0.0,\n            'user_embeddings': torch.zeros(100, 50),\n            'item_embeddings': torch.zeros(50, 50)\n        }\n\n\nif __name__ == \"__main__\":\n    print(\"Testing HRCF implementation...\")\n    result = run_hrcf_experiment()\n    print(f\"Final Recall@10: {result:.4f}\")\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["To improve the HRCF (Hyperbolic Regularized Collaborative Filtering) algorithm, we will focus on enhancing the use of hyperbolic space operations and the efficiency of the training process. Here are the changes I suggest:\n\n1. **Optimize Hyperbolic Manifold Operations:** We can improve the efficiency of manifold operations like `expmap` and `logmap` by ensuring they are implemented with minimal computational overhead.\n\n2. **Fine-tune the Fermi-Dirac Decoder:** Dynamically adjusting the temperature parameter in the Fermi-Dirac decoder can potentially improve convergence.\n\n3. **Adjust Geometric Regularization:** Tuning the geometric regularization term can help in better embedding space utilization.\n\n4. **Reduce Execution Time:** By optimizing the training loop and leveraging batched operations, we can potentially reduce the execution time.\n\nHere are the specific code changes using the SEARCH/REPLACE format:\n\n### Change 1: Optimize `expmap` and `logmap` operations\n\nThe `expmap` and `logmap` functions in the `Hyperboloid` class can be optimized by reducing redundant operations and ensuring that they use efficient tensor operations.\n\n```python\n<<<<<<< SEARCH\ndef expmap(self, u, x, c):\n    K = 1. / c\n    sqrtK = K ** 0.5\n    normu = self.minkowski_norm(u)\n    normu = torch.clamp(normu, max=self.max_norm)\n    theta = normu / sqrtK\n    theta = torch.clamp(theta, min=self.min_norm)\n    result = cosh(theta) * x + sinh(theta) * u / theta\n    return self.proj(result, c)\n\ndef logmap(self, x, y, c):\n    K = 1. / c\n    xy = torch.clamp(self.minkowski_dot(x, y) + K, max=-self.eps[x.dtype]) - K\n    u = y + xy * x * c\n    normu = self.minkowski_norm(u)\n    normu = torch.clamp(normu, min=self.min_norm)\n    dist = self.sqdist(x, y, c) ** 0.5\n    result = dist * u / normu\n    return self.proj_tan(result, x, c)\n=======\n# Optimized expmap and logmap functions\ndef expmap(self, u, x, c):\n    K = 1. / c\n    normu = torch.clamp(self.minkowski_norm(u), max=self.max_norm)\n    theta = torch.clamp(normu / K**0.5, min=self.min_norm)\n    result = cosh(theta) * x + sinh(theta) * u / theta\n    return self.proj(result, c)\n\ndef logmap(self, x, y, c):\n    K = 1. / c\n    xy = self.minkowski_dot(x, y)\n    u = y + (xy + K) * x * c\n    normu = torch.clamp(self.minkowski_norm(u), min=self.min_norm)\n    dist = self.sqdist(x, y, c).sqrt()\n    return self.proj_tan(dist * u / normu, x, c)\n>>>>>>> REPLACE\n```\n\n### Change 2: Improve Fermi-Dirac Decoder\n\nRefine the dynamic temperature adjustment in the `FermiDiracDecoder` to make it more responsive to the variance in distances.\n\n```python\n<<<<<<< SEARCH\ndef forward(self, dist):\n    # Adjust temperature dynamically\n    dynamic_t = self.t * torch.std(dist) + 1e-4\n    probs = 1. / (torch.exp((dist - self.r) / dynamic_t) + 1)\n=======\n# Improved dynamic temperature adjustment\ndef forward(self, dist):\n    # Improved dynamic temperature calculation\n    dynamic_t = self.t * (torch.std(dist) / (torch.mean(dist) + 1e-4)) + 1e-4\n    probs = 1. / (torch.exp((dist - self.r) / dynamic_t) + 1)\n>>>>>>> REPLACE\n```\n\n### Change 3: Fine-tune Geometric Regularization\n\nAdjust the geometric regularizer to better control the norm of the item embeddings.\n\n```python\n<<<<<<< SEARCH\ndef geometric_regularizer(self, embeddings):\n    embeddings_tan = self.manifold.logmap0(embeddings, c=1.0)\n    item_embeddings = embeddings_tan[self.num_users:]\n    item_mean_norm = ((1e-6 + item_embeddings.pow(2).sum(dim=1)).mean()).sqrt()\n    return 1.0 / item_mean_norm\n=======\n# Fine-tuned geometric regularization\ndef geometric_regularizer(self, embeddings):\n    embeddings_tan = self.manifold.logmap0(embeddings, c=1.0)\n    item_embeddings = embeddings_tan[self.num_users:]\n    item_mean_norm = torch.mean(torch.norm(item_embeddings, dim=1))\n    return 1.0 / (item_mean_norm + 1e-4)\n>>>>>>> REPLACE\n```\n\n### Change 4: Reduce Execution Time\n\nOptimize the training loop to handle batched operations more efficiently.\n\n```python\n<<<<<<< SEARCH\nfor batch in range(num_batches):\n    triples = sampler.next_batch()\n    model.train()\n    optimizer.zero_grad()\n    embeddings = model.encode(data.adj_train_norm)\n    train_loss = model.compute_loss(embeddings, torch.LongTensor(triples))\n=======\n# Improved training loop with more efficient batching\nfor batch in range(num_batches):\n    triples = sampler.next_batch()\n    optimizer.zero_grad()\n    embeddings = model.encode(data.adj_train_norm)\n    train_loss = model.compute_loss(embeddings, torch.LongTensor(triples).to(default_device()))\n>>>>>>> REPLACE\n```\n\nThese changes aim to leverage the properties of hyperbolic space more effectively, improve the decoder's adaptability, and optimize the regularization and training loop to enhance performance and reduce execution time."]}}}