{"id": "4c3385fa-ce1f-46e6-81d9-76f5bfe85bcc", "code": "# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\nimport math\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.module import Module\nfrom torch.nn.parameter import Parameter\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom sklearn.model_selection import train_test_split\n\n\n# --- From hgcn_utils/math_utils.py ---\ndef cosh(x, clamp=15):\n    return x.clamp(-clamp, clamp).cosh()\n\ndef sinh(x, clamp=15):\n    return x.clamp(-clamp, clamp).sinh()\n\ndef tanh(x, clamp=15):\n    return x.clamp(-clamp, clamp).tanh()\n\nclass Artanh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 - input ** 2)\n\nclass Arsinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(1 + z.pow(2))).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 + input ** 2) ** 0.5\n\nclass Arcosh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(min=1.0 + 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(z.pow(2) - 1)).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (input ** 2 - 1) ** 0.5\n\ndef arcosh(x):\n    return Arcosh.apply(x)\n\ndef arsinh(x):\n    return Arsinh.apply(x)\n\ndef artanh(x):\n    return Artanh.apply(x)\n\n\n# --- Helper functions ---\ndef default_device() -> torch.device:\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix.\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo()\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n    )\n    values = torch.Tensor(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\n# --- From manifolds/base.py ---\nclass Manifold(object):\n    def __init__(self):\n        super().__init__()\n        self.eps = 10e-8\n\n    def sqdist(self, p1, p2, c):\n        raise NotImplementedError\n\n    def egrad2rgrad(self, p, dp, c):\n        raise NotImplementedError\n\n    def proj(self, p, c):\n        raise NotImplementedError\n\n    def proj_tan(self, u, p, c):\n        raise NotImplementedError\n\n    def proj_tan0(self, u, c):\n        raise NotImplementedError\n\n    def expmap(self, u, p, c):\n        raise NotImplementedError\n\n    def logmap(self, p1, p2, c):\n        raise NotImplementedError\n\n    def expmap0(self, u, c):\n        raise NotImplementedError\n\n    def logmap0(self, p, c):\n        raise NotImplementedError\n\n    def mobius_add(self, x, y, c, dim=-1):\n        raise NotImplementedError\n\n    def mobius_matvec(self, m, x, c):\n        raise NotImplementedError\n\n    def init_weights(self, w, c, irange=1e-5):\n        raise NotImplementedError\n\n    def inner(self, p, c, u, v=None, keepdim=False):\n        raise NotImplementedError\n\n    def ptransp(self, x, y, u, c):\n        raise NotImplementedError\n\n    def ptransp0(self, x, u, c):\n        raise NotImplementedError\n\nclass ManifoldParameter(Parameter):\n    def __new__(cls, data, requires_grad, manifold, c):\n        return Parameter.__new__(cls, data, requires_grad)\n\n    def __init__(self, data, requires_grad, manifold, c):\n        self.c = c\n        self.manifold = manifold\n\n    def __repr__(self):\n        return '{} Parameter containing:\\n'.format(self.manifold.name) + super(Parameter, self).__repr__()\n\n\n# --- From manifolds/hyperboloid.py ---\nclass Hyperboloid(Manifold):\n    def __init__(self):\n        super(Hyperboloid, self).__init__()\n        self.name = 'Hyperboloid'\n        self.eps = {torch.float32: 1e-7, torch.float64: 1e-15}\n        self.min_norm = 1e-15\n        self.max_norm = 1e6\n\n    def minkowski_dot(self, x, y, keepdim=True):\n        res = torch.sum(x * y, dim=-1) - 2 * x[..., 0] * y[..., 0]\n        if keepdim:\n            res = res.view(res.shape + (1,))\n        return res\n\n    def minkowski_norm(self, u, keepdim=True):\n        dot = self.minkowski_dot(u, u, keepdim=keepdim)\n        return torch.sqrt(torch.clamp(dot, min=self.eps[u.dtype]))\n\n    def sqdist(self, x, y, c):\n        K = 1. / c\n        prod = self.minkowski_dot(x, y)\n        theta = torch.clamp(-prod / K, min=1.0 + self.eps[x.dtype])\n        sqdist = K * arcosh(theta) ** 2\n        return torch.clamp(sqdist, max=50.0)\n\n    def proj(self, x, c):\n        # Project points to the hyperboloid manifold (numerically stable).\n        K = 1. / c\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d)\n        y_sqnorm = torch.sum(y ** 2, dim=-1, keepdim=True)\n        time_component = torch.sqrt(torch.clamp(K + y_sqnorm, min=self.eps[x.dtype]))\n        x_new = torch.cat([time_component, y], dim=-1)\n        return x_new\n\n    def proj_tan(self, u, x, c):\n        d = x.size(1) - 1\n        ux = torch.sum(x.narrow(-1, 1, d) * u.narrow(-1, 1, d), dim=1, keepdim=True)\n        mask = torch.ones_like(u)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(u)\n        vals[:, 0:1] = ux / torch.clamp(x[:, 0:1], min=self.eps[x.dtype])\n        return vals + mask * u\n\n    def expmap(self, u, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, max=self.max_norm)\n        theta = normu / sqrtK\n        theta = torch.clamp(theta, min=self.min_norm)\n        result = cosh(theta) * x + sinh(theta) * u / theta\n        return self.proj(result, c)\n\n    def logmap(self, x, y, c):\n        K = 1. / c\n        xy = torch.clamp(self.minkowski_dot(x, y) + K, max=-self.eps[x.dtype]) - K\n        u = y + xy * x * c\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, min=self.min_norm)\n        dist = self.sqdist(x, y, c) ** 0.5\n        result = dist * u / normu\n        return self.proj_tan(result, x, c)\n\n    def expmap0(self, u, c):\n        # Exponential map at origin for the hyperboloid\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = u.size(-1) - 1\n        x = u.narrow(-1, 1, d).contiguous().view(-1, d)\n        x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n        x_norm = torch.clamp(x_norm, min=self.min_norm)\n        theta = x_norm / sqrtK\n        res = torch.zeros(u.size(0), d + 1, device=u.device, dtype=u.dtype)\n        res[:, 0:1] = sqrtK * cosh(theta)\n        res[:, 1:] = sqrtK * sinh(theta) * x / x_norm\n        return self.proj(res, c)\n\n    def logmap0(self, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d).view(-1, d)\n        y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n        y_norm = torch.clamp(y_norm, min=self.min_norm)\n        res = torch.zeros_like(x)\n        theta = torch.clamp(x[:, 0:1] / sqrtK, min=1.0 + self.eps[x.dtype])\n        res[:, 1:] = sqrtK * arcosh(theta) * y / y_norm\n        return res\n\n    def ptransp(self, x, y, u, c):\n        logxy = self.logmap(x, y, c)\n        logyx = self.logmap(y, x, c)\n        sqdist = torch.clamp(self.sqdist(x, y, c), min=self.min_norm)\n        alpha = self.minkowski_dot(logxy, u) / sqdist\n        res = u - alpha * (logxy + logyx)\n        return self.proj_tan(res, y, c)\n\n    def egrad2rgrad(self, x, grad, k, dim=-1):\n        grad.narrow(-1, 0, 1).mul_(-1)\n        grad = grad.addcmul(self.inner(x, grad, dim=dim, keepdim=True), x / k)\n        return grad\n\n    def inner(self, u, v, keepdim: bool = False, dim: int = -1):\n        d = u.size(dim) - 1\n        uv = u * v\n        if keepdim is False:\n            return -uv.narrow(dim, 0, 1).sum(dim=dim, keepdim=False) + uv.narrow(\n                dim, 1, d\n            ).sum(dim=dim, keepdim=False)\n        else:\n            return torch.cat((-uv.narrow(dim, 0, 1), uv.narrow(dim, 1, d)), dim=dim).sum(\n                dim=dim, keepdim=True\n            )\n\n\n# --- RiemannianSGD Optimizer ---\n_default_manifold = Hyperboloid()\n\ndef copy_or_set_(dest, source):\n    if dest.stride() != source.stride():\n        return dest.copy_(source)\n    else:\n        return dest.set_(source)\n\nclass OptimMixin(object):\n    def __init__(self, *args, stabilize=None, **kwargs):\n        self._stabilize = stabilize\n        super().__init__(*args, **kwargs)\n\n    def stabilize_group(self, group):\n        pass\n\n    def stabilize(self):\n        for group in self.param_groups:\n            self.stabilize_group(group)\n\nclass RiemannianSGD(OptimMixin, torch.optim.SGD):\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        with torch.no_grad():\n            for group in self.param_groups:\n                if \"step\" not in group:\n                    group[\"step\"] = 0\n                weight_decay = group[\"weight_decay\"]\n                momentum = group[\"momentum\"]\n                dampening = group[\"dampening\"]\n                nesterov = group[\"nesterov\"]\n                learning_rate = group[\"lr\"]\n\n                for point in group[\"params\"]:\n                    if isinstance(point, ManifoldParameter):\n                        manifold = point.manifold\n                        c = point.c\n                    else:\n                        manifold = _default_manifold\n                        c = 1.\n\n                    grad = point.grad\n                    if grad is None:\n                        continue\n                    if grad.is_sparse:\n                        raise RuntimeError(\"RiemannianSGD does not support sparse gradients\")\n                    \n                    state = self.state[point]\n                    if len(state) == 0:\n                        if momentum > 0:\n                            state[\"momentum_buffer\"] = grad.clone()\n\n                    grad.add_(weight_decay, point)\n                    grad = manifold.egrad2rgrad(point, grad, c)\n\n                    if momentum > 0:\n                        momentum_buffer = state[\"momentum_buffer\"]\n                        momentum_buffer.mul_(momentum).add_(1 - dampening, grad)\n                        if nesterov:\n                            grad = grad.add_(momentum, momentum_buffer)\n                        else:\n                            grad = momentum_buffer\n\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        new_momentum_buffer = manifold.ptransp(point, new_point, momentum_buffer, c)\n                        momentum_buffer.set_(new_momentum_buffer)\n                        copy_or_set_(point, new_point)\n                    else:\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        copy_or_set_(point, new_point)\n\n                    group[\"step\"] += 1\n                if self._stabilize is not None and group[\"step\"] % self._stabilize == 0:\n                    self.stabilize_group(group)\n        return loss\n\n    @torch.no_grad()\n    def stabilize_group(self, group):\n        for p in group[\"params\"]:\n            if not isinstance(p, ManifoldParameter):\n                continue\n            manifold = p.manifold\n            momentum = group[\"momentum\"]\n            copy_or_set_(p, manifold.proj(p))\n            if momentum > 0:\n                param_state = self.state[p]\n                if not param_state:\n                    continue\n                if \"momentum_buffer\" in param_state:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.set_(manifold.proju(p, buf))\n\n\n# --- WarpSampler ---\ndef sample_function(adj_train, num_nodes, batch_size, n_negative, result_queue):\n    num_users, num_items = num_nodes\n    adj_train = lil_matrix(adj_train)\n    all_pairs = np.asarray(adj_train.nonzero()).T\n    user_item_pairs = all_pairs[: adj_train.count_nonzero() // 2]\n    item_user_pairs = all_pairs[adj_train.count_nonzero() // 2:]\n\n    assert len(user_item_pairs) == len(item_user_pairs)\n    np.random.shuffle(user_item_pairs)\n    np.random.shuffle(item_user_pairs)\n\n    all_pairs_set = {idx: set(row) for idx, row in enumerate(adj_train.rows)}\n    user_item_pairs_set = dict(itertools.islice(all_pairs_set.items(), num_users))\n\n    while True:\n        for i in range(int(len(user_item_pairs) / batch_size)):\n            samples_for_users = batch_size\n            user_positive_items_pairs = user_item_pairs[i * samples_for_users: (i + 1) * samples_for_users, :]\n            user_negative_samples = np.random.randint(num_users, sum(num_nodes), size=(samples_for_users, n_negative))\n\n            for user_positive, negatives, i in zip(user_positive_items_pairs,\n                                                   user_negative_samples,\n                                                   range(len(user_negative_samples))):\n                user = user_positive[0]\n                for j, neg in enumerate(negatives):\n                    while neg in user_item_pairs_set[user]:\n                        user_negative_samples[i, j] = neg = np.random.randint(num_users, sum(num_nodes))\n\n            user_triples = np.hstack((user_positive_items_pairs, user_negative_samples))\n            np.random.shuffle(user_triples)\n            result_queue.put(user_triples)\n\nclass WarpSampler(object):\n    def __init__(self, num_nodes, user_item_matrix, batch_size=10000, n_negative=1, n_workers=5):\n        self.result_queue = Queue(maxsize=n_workers * 2)\n        self.processors = []\n        for i in range(n_workers):\n            self.processors.append(\n                Process(target=sample_function, args=(user_item_matrix,\n                                                      num_nodes,\n                                                      batch_size,\n                                                      n_negative,\n                                                      self.result_queue)))\n            self.processors[-1].start()\n\n    def next_batch(self):\n        return self.result_queue.get()\n\n    def close(self):\n        for p in self.processors:\n            p.terminate()\n            p.join()\n\n\n# --- Layers ---\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t=0.3):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = t  # Directly set t to a lower value for sharper transition\n\n    def forward(self, dist):\n        # Use a numerically stable sigmoid for better performance\n        probs = torch.sigmoid(-(dist - self.r) / self.t)\n        return probs\n\nclass StackGCNs(Module):\n    def __init__(self, num_layers):\n        super(StackGCNs, self).__init__()\n        self.num_gcn_layers = num_layers - 1\n\n    def plainGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return output[-1]\n\n    def resSumGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return sum(output[1:])\n\nclass HypAgg(Module):\n    def __init__(self, manifold, c, in_features, network, num_layers):\n        super(HypAgg, self).__init__()\n        self.manifold = manifold\n        self.c = c\n        self.in_features = in_features\n        self.stackGCNs = getattr(StackGCNs(num_layers), network)\n\n    def forward(self, x, adj):\n        x_tangent = self.manifold.logmap0(x, c=self.c)\n        output = self.stackGCNs((x_tangent, adj))\n        output = output - output.mean(dim=0)\n        output = self.manifold.proj(self.manifold.expmap0(output, c=self.c), c=self.c)\n        return output\n\n    def extra_repr(self):\n        return 'c={}'.format(self.c)\n\nclass HyperbolicGraphConvolution(nn.Module):\n    def __init__(self, manifold, in_features, out_features, c_in, network, num_layers):\n        super(HyperbolicGraphConvolution, self).__init__()\n        self.agg = HypAgg(manifold, c_in, out_features, network, num_layers)\n\n    def forward(self, input):\n        x, adj = input\n        h = self.agg.forward(x, adj)\n        output = h, adj\n        return output\n\nclass Encoder(nn.Module):\n    def __init__(self, c):\n        super(Encoder, self).__init__()\n        self.c = c\n\n    def encode(self, x, adj):\n        if hasattr(self, 'encode_graph') and self.encode_graph:\n            input = (x, adj)\n            output, _ = self.layers.forward(input)\n        else:\n            output = self.layers.forward(x)\n        return output\n\nclass HRCF(Encoder):\n    def __init__(self, c, args):\n        super(HRCF, self).__init__(c)\n        self.manifold = Hyperboloid()\n        assert args.num_layers > 1\n\n        hgc_layers = []\n        in_dim = out_dim = args.embedding_dim\n        hgc_layers.append(\n            HyperbolicGraphConvolution(\n                self.manifold, in_dim, out_dim, self.c, args.network, args.num_layers\n            )\n        )\n        self.layers = nn.Sequential(*hgc_layers)\n        self.encode_graph = True\n\n    def encode(self, x, adj):\n        x_hyp = self.manifold.proj(x, c=self.c)\n        return super(HRCF, self).encode(x_hyp, adj)\n\n\n# --- Data Generator ---\nclass Data(object):\n    def __init__(self, dataset, norm_adj, seed, test_ratio, data_path=\"./data\"):\n        pkl_path = os.path.join(data_path, dataset)\n        self.pkl_path = pkl_path\n        self.dataset = dataset\n        \n        # Load data\n        user_item_list_path = os.path.join(pkl_path, 'user_item_list.pkl')\n        if os.path.exists(user_item_list_path):\n            self.user_item_list = self.load_pickle(user_item_list_path)\n        else:\n            # Create synthetic data if real data not available\n            self.user_item_list = self.create_synthetic_data()\n        \n        self.train_dict, self.test_dict = self.split_data_randomly(self.user_item_list, test_ratio, seed)\n        self.num_users, self.num_items = len(self.user_item_list), max([max(x) for x in self.user_item_list]) + 1\n\n        self.adj_train, user_item = self.generate_adj()\n\n        if eval(norm_adj):\n            self.adj_train_norm = normalize(self.adj_train + sp.eye(self.adj_train.shape[0]))\n            self.adj_train_norm = sparse_mx_to_torch_sparse_tensor(self.adj_train_norm)\n\n        print('num_users %d, num_items %d' % (self.num_users, self.num_items))\n        print('adjacency matrix shape: ', self.adj_train.shape)\n\n        tot_num_rating = sum([len(x) for x in self.user_item_list])\n        print('number of all ratings {}, density {:.6f}'.format(tot_num_rating,\n                                                                tot_num_rating / (self.num_users * self.num_items)))\n\n        self.user_item_csr = self.generate_rating_matrix([*self.train_dict.values()], self.num_users, self.num_items)\n\n    def create_synthetic_data(self):\n        \"\"\"Create synthetic user-item interactions for testing\"\"\"\n        np.random.seed(42)\n        num_users, num_items = 100, 50\n        user_item_list = []\n        \n        for user_id in range(num_users):\n            num_interactions = np.random.randint(5, 20)\n            items = np.random.choice(num_items, num_interactions, replace=False)\n            user_item_list.append(items.tolist())\n        \n        return user_item_list\n\n    def generate_adj(self):\n        user_item = np.zeros((self.num_users, self.num_items)).astype(int)\n        for i, v in self.train_dict.items():\n            user_item[i][v] = 1\n        coo_user_item = sp.coo_matrix(user_item)\n\n        start = time.time()\n        print('generating adj csr... ')\n        rows = np.concatenate((coo_user_item.row, coo_user_item.transpose().row + self.num_users))\n        cols = np.concatenate((coo_user_item.col + self.num_users, coo_user_item.transpose().col))\n        data = np.ones((coo_user_item.nnz * 2,))\n        adj_csr = sp.coo_matrix((data, (rows, cols))).tocsr().astype(np.float32)\n        print('time elapsed: {:.3f}'.format(time.time() - start))\n        return adj_csr, user_item\n\n    def load_pickle(self, name):\n        with open(name, 'rb') as f:\n            return pkl.load(f, encoding='latin1')\n\n    def split_data_randomly(self, user_records, test_ratio, seed):\n        train_dict = {}\n        test_dict = {}\n        for user_id, item_list in enumerate(user_records):\n            tmp_train_sample, tmp_test_sample = train_test_split(item_list, test_size=test_ratio, random_state=seed)\n\n            train_sample = []\n            for place in item_list:\n                if place not in tmp_test_sample:\n                    train_sample.append(place)\n\n            test_sample = []\n            for place in tmp_test_sample:\n                test_sample.append(place)\n\n            train_dict[user_id] = train_sample\n            test_dict[user_id] = test_sample\n        return train_dict, test_dict\n\n    def generate_rating_matrix(self, train_set, num_users, num_items):\n        row = []\n        col = []\n        data = []\n        for user_id, article_list in enumerate(train_set):\n            for article in article_list:\n                row.append(user_id)\n                col.append(article)\n                data.append(1)\n\n        row = np.array(row)\n        col = np.array(col)\n        data = np.array(data)\n        rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n        return rating_matrix\n\n\n# EVOLVE-BLOCK-START\nclass HRCFModel(nn.Module):\n    def __init__(self, users_items, args):\n        super(HRCFModel, self).__init__()\n        self.c = torch.tensor([args.c]).to(default_device())\n        self.manifold = Hyperboloid()\n        self.nnodes = args.n_nodes\n        self.encoder = HRCF(self.c, args)\n        self.num_users, self.num_items = users_items\n        self.margin = args.margin\n        self.weight_decay = args.weight_decay\n        self.num_layers = args.num_layers\n        self.args = args\n        self.dc = FermiDiracDecoder(r=args.r, t=args.t)\n        self.embedding = nn.Embedding(num_embeddings=self.num_users + self.num_items,\n                                      embedding_dim=args.embedding_dim).to(default_device())\n        \n        # Use a smaller scale for better initialization in hyperbolic space\n        self.embedding.state_dict()['weight'].uniform_(-args.scale * 0.5, args.scale * 0.5)\n        self.embedding.weight = nn.Parameter(self.manifold.expmap0(self.embedding.state_dict()['weight'], self.c))\n        self.embedding.weight = ManifoldParameter(self.embedding.weight, True, self.manifold, self.c)\n        self.alpha = args.alpha\n\n    def encode(self, adj):\n        x = self.embedding.weight\n        if torch.cuda.is_available():\n            adj = adj.to(default_device())\n            x = x.to(default_device())\n        h = self.encoder.encode(x, adj)\n        return h\n\n    def decode(self, h, idx):\n        emb_in = h[idx[:, 0], :]\n        emb_out = h[idx[:, 1], :]\n        sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n        probs = self.dc.forward(sqdist)\n        return [sqdist, probs]\n\n    def geometric_regularizer(self, embeddings):\n        # Enhanced geometric regularizer: penalize collapse and excessive clustering in hyperbolic space.\n        embeddings_tan = self.manifold.logmap0(embeddings, c=self.c)\n        item_embeddings = embeddings_tan[self.num_users:]\n        user_embeddings = embeddings_tan[:self.num_users]\n        # Encourage norms toward 1.0, but also encourage spread (maximize variance)\n        item_norms = torch.norm(item_embeddings, p=2, dim=1)\n        user_norms = torch.norm(user_embeddings, p=2, dim=1)\n        mean_item_norm = item_norms.mean()\n        mean_user_norm = user_norms.mean()\n        # Encourage norms toward 1, and also encourage spread (variance regularization)\n        reg = (mean_item_norm - 1.0).abs() + (mean_user_norm - 1.0).abs()\n        # Penalize embeddings that are too close to each other in hyperbolic space\n        # Refined pairwise penalty to focus only on extremely close pairs\n        pairwise_distances = torch.cdist(item_embeddings, item_embeddings, p=2)\n        # Increase penalty for extremely close pairs to avoid collapse\n        close_pair_penalty = torch.where(pairwise_distances < 0.1, torch.exp(-pairwise_distances * 10), torch.zeros_like(pairwise_distances))\n        pairwise_penalty = close_pair_penalty.mean()\n\n        # Encourage diversity by maximizing distance variance with a stronger emphasis\n        distance_variance = torch.var(pairwise_distances)\n\n        reg += 0.15 * (1.0 / (item_norms.std() + 1e-6) + 1.0 / (user_norms.std() + 1e-6)) + 0.02 * pairwise_penalty - 0.02 * distance_variance\n        return reg\n\n    def ranking_loss(self, pos_sqdist, neg_sqdist):\n        # Use log-sigmoid for more stable pairwise ranking loss in hyperbolic space.\n        margin_loss = -F.logsigmoid(neg_sqdist - pos_sqdist + self.margin)\n        return margin_loss.mean()\n\n    def compute_loss(self, embeddings, triples):\n        train_edges = triples[:, [0, 1]]\n        sampled_false_edges_list = [triples[:, [0, 2 + i]] for i in range(self.args.num_neg)]\n\n        pos_sqdist, _ = self.decode(embeddings, train_edges)\n        neg_losses = []\n        for neg_edges in sampled_false_edges_list:\n            neg_sqdist, _ = self.decode(embeddings, neg_edges)\n            neg_losses.append(self.ranking_loss(pos_sqdist, neg_sqdist))\n        total_neg_loss = torch.stack(neg_losses).mean()\n\n        gr_loss = self.geometric_regularizer(embeddings)\n        # Add small L2 norm penalty on tangent vectors for further stability\n        l2_reg = embeddings.norm(p=2) * 1e-4\n\n        return total_neg_loss + self.alpha * gr_loss + l2_reg\n\n    def predict(self, h, data):\n        # Efficient batched scoring to save memory and avoid OOM\n        num_users, num_items = data.num_users, data.num_items\n        user_emb = h[:num_users]  # [num_users, d]\n        item_emb = h[num_users:num_users+num_items]  # [num_items, d]\n        scores = np.zeros((num_users, num_items))\n        batch_size = 512  # Increase batch size to improve prediction efficiency\n        with torch.no_grad():\n            for start in range(0, num_users, batch_size):\n                end = min(start + batch_size, num_users)\n                u_emb = user_emb[start:end]  # [batch, d]\n                # Repeat for all items\n                u_emb_expand = u_emb.unsqueeze(1).expand(-1, num_items, -1).contiguous().view(-1, u_emb.size(-1))\n                i_emb_expand = item_emb.unsqueeze(0).expand(end-start, num_items, -1).contiguous().view(-1, item_emb.size(-1))\n                sqdist = self.manifold.sqdist(u_emb_expand, i_emb_expand, self.c)\n                s = -sqdist.view(end-start, num_items).cpu().numpy()\n                scores[start:end] = s\n        return scores\n# EVOLVE-BLOCK-END\n\n\n# --- Evaluation metrics ---\ndef recall_at_k(test_dict, pred_list, k):\n    recall_list = []\n    for user_id in test_dict:\n        if user_id < len(pred_list):\n            test_items = set(test_dict[user_id])\n            pred_items = set(pred_list[user_id][:k])\n            if len(test_items) > 0:\n                recall = len(test_items & pred_items) / len(test_items)\n                recall_list.append(recall)\n    return np.mean(recall_list) if recall_list else 0.0\n\ndef ndcg_func(ground_truths, ranks):\n    result = 0\n    for i, (rank, ground_truth) in enumerate(zip(ranks, ground_truths)):\n        len_rank = len(rank)\n        len_gt = len(ground_truth)\n        idcg_len = min(len_gt, len_rank)\n\n        idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\n        idcg[idcg_len:] = idcg[idcg_len-1]\n\n        dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\n        result += dcg / idcg\n    return result / len(ranks)\n\ndef eval_rec(pred_matrix, data):\n    topk = 50\n    pred_matrix[data.user_item_csr.nonzero()] = -np.inf\n    ind = np.argpartition(pred_matrix, -topk)\n    ind = ind[:, -topk:]\n    arr_ind = pred_matrix[np.arange(len(pred_matrix))[:, None], ind]\n    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(pred_matrix)), ::-1]\n    pred_list = ind[np.arange(len(pred_matrix))[:, None], arr_ind_argsort]\n    \n    recall = []\n    for k in [5, 10, 20, 50]:\n        recall.append(recall_at_k(data.test_dict, pred_list, k))\n\n    all_ndcg = ndcg_func([*data.test_dict.values()], pred_list)\n    ndcg = [all_ndcg[x-1] for x in [5, 10, 20, 50]]\n\n    return recall, ndcg\n\n\n# --- Main experiment functions ---\ndef run_hrcf_experiment():\n    \"\"\"Main experiment function matching original HRCF exactly\"\"\"\n    try:\n        # Set up arguments exactly like original HRCF Amazon-CD\n        class Args:\n            def __init__(self):\n                # Exact Amazon-CD parameters from run_cd.sh\n                self.embedding_dim = 50\n                self.dim = 50\n                self.num_layers = 8  # Critical: was 4, now 8\n                self.c = 1.0\n                self.margin = 0.12  # Fine-tuned margin for better separation\n                self.weight_decay = 5e-3  # Critical: was 0.01, now 5e-3\n                self.alpha = 25  # Critical: was 20, now 25\n                self.r = 2.0\n                self.t = 1.0\n                self.scale = 0.1\n                self.num_neg = 1\n                self.network = 'resSumGCN'\n                self.lr = 0.003  # Further increase learning rate for more aggressive updates\n                self.momentum = 0.9  # Slightly reduce momentum to prevent overshooting\n                self.batch_size = 10000  # Critical: was small, now 10000\n                self.epochs = 100  # Increased for better training\n                self.seed = 1234\n                self.test_ratio = 0.2\n                self.norm_adj = 'True'\n        \n        args = Args()\n        \n        # Prepare data exactly like original\n        data = Data(args.dataset if hasattr(args, 'dataset') else 'Amazon-CD', \n                   args.norm_adj, args.seed, args.test_ratio)\n        args.n_nodes = data.num_users + data.num_items\n        args.feat_dim = args.embedding_dim\n        \n        # Initialize model\n        model = HRCFModel((data.num_users, data.num_items), args)\n        model = model.to(default_device())\n        \n        # Use RiemannianSGD exactly like original\n        optimizer = RiemannianSGD(params=model.parameters(), lr=args.lr,\n                                  weight_decay=args.weight_decay, momentum=args.momentum)\n        \n        # Initialize WarpSampler exactly like original\n        sampler = WarpSampler((data.num_users, data.num_items), data.adj_train, \n                             args.batch_size, args.num_neg)\n        \n        num_pairs = data.adj_train.count_nonzero() // 2\n        num_batches = int(num_pairs / args.batch_size) + 1\n        print(f\"Number of batches per epoch: {num_batches}\")\n        \n        # Training loop exactly like original\n        for epoch in range(1, args.epochs + 1):\n            print(f\"--> Starting epoch {epoch}\")\n            avg_loss = 0.\n            t = time.time()\n            \n            for batch in range(num_batches):\n                triples = sampler.next_batch()\n                model.train()\n                optimizer.zero_grad()\n                embeddings = model.encode(data.adj_train_norm)\n                train_loss = model.compute_loss(embeddings, torch.LongTensor(triples))\n                \n                if torch.isnan(train_loss):\n                    print('NaN loss detected!')\n                    break\n                    \n                train_loss.backward()\n                optimizer.step()\n                avg_loss += train_loss / num_batches\n            \n            avg_loss = avg_loss.detach().cpu().numpy()\n            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Time: {time.time() - t:.4f}s\")\n            \n            # Evaluation every 10 epochs\n            if epoch % 10 == 0:\n                model.eval()\n                with torch.no_grad():\n                    embeddings = model.encode(data.adj_train_norm)\n                    pred_matrix = model.predict(embeddings, data)\n                    recall, ndcg = eval_rec(pred_matrix, data)\n                    print(f\"Recall@10: {recall[1]:.4f}, NDCG@10: {ndcg[1]:.4f}\")\n        \n        sampler.close()\n        \n        # Final evaluation\n        model.eval()\n        with torch.no_grad():\n            embeddings = model.encode(data.adj_train_norm)\n            pred_matrix = model.predict(embeddings, data)\n            recall, ndcg = eval_rec(pred_matrix, data)\n            \n        return recall[1]  # Return Recall@10\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment: {e}\")\n        traceback.print_exc()\n        return 0.0\n\n\ndef run_hrcf_experiment_with_embeddings():\n    \"\"\"Extended experiment that returns embeddings\"\"\"\n    try:\n        recall_score = run_hrcf_experiment()\n        \n        # Return dummy embeddings for compatibility\n        num_users, num_items = 100, 50\n        embedding_dim = 50\n        \n        user_embeddings = torch.randn(num_users, embedding_dim) * 0.1\n        item_embeddings = torch.randn(num_items, embedding_dim) * 0.1\n        \n        return {\n            'score': recall_score,\n            'user_embeddings': user_embeddings,\n            'item_embeddings': item_embeddings\n        }\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment_with_embeddings: {e}\")\n    return {\n            'score': 0.0,\n            'user_embeddings': torch.zeros(100, 50),\n            'item_embeddings': torch.zeros(50, 50)\n        }\n\n\nif __name__ == \"__main__\":\n    print(\"Testing HRCF implementation...\")\n    result = run_hrcf_experiment()\n    print(f\"Final Recall@10: {result:.4f}\")\n", "language": "python", "parent_id": "263448f1-520f-401d-9173-1dcab56723d6", "generation": 6, "timestamp": 1751232164.5515175, "iteration_found": 232, "metrics": {"recall_at_10": 0.02094207542182536, "recall_at_20": 0.03442003233191132, "precision_at_10": 0.008001045888351196, "precision_at_20": 0.006606528086460081, "ndcg_at_10": 0.01622825381575051, "ndcg_at_20": 0.02060384851434349, "hit_rate_at_10": 0.07055388503943871, "hit_rate_at_20": 0.1069856626138493, "execution_time": 545.9297635555267, "combined_score": 0.02383065597403855, "error": null}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 9 lines with 10 lines\nChange 2: Replace 8 lines with 9 lines\nChange 3: Replace 2 lines with 2 lines", "parent_metrics": {"recall_at_10": 0.021833698839545442, "recall_at_20": 0.03477458591579266, "precision_at_10": 0.008035908833398482, "precision_at_20": 0.006560770471085507, "ndcg_at_10": 0.016821831246754997, "ndcg_at_20": 0.02106502022533726, "hit_rate_at_10": 0.07072819976467512, "hit_rate_at_20": 0.10715997733908572, "execution_time": 531.5867323875427, "combined_score": 0.024369146989747358, "error": null}, "island": 3}, "artifacts_json": "{\"success_summary\": \"Successful HRCF training with 100 epochs\", \"model_architecture\": \"Embedding dim: 50, Layers: 4, Network: resSumGCN\", \"training_details\": \"Batch size: 10000, LR: 0.001, Weight decay: 0.005\", \"dataset_info\": \"22947 users, 18395 items, 657558 interactions\", \"performance_analysis\": \"Combined score: 0.0238 (Target: >0.10 for good performance)\", \"metric_breakdown\": \"Recall@10: 0.0209, NDCG@10: 0.0162, Precision@10: 0.0080\", \"execution_efficiency\": \"Training time: 545.9s (5.5s per epoch)\", \"recommendation_quality\": \"Moderate\", \"embedding_analysis\": \"Effective rank: 48.0/50 (0.96)\"}", "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert having 10 years experiences in recommendation systems and hyperbolic geometry. Your task is to improve the HRCF (Hyperbolic Regularized Collaborative Filtering) algorithm. Focus on optimizing the loss functions, geometric regularization techniques, and hyperbolic space operations to enhance recommendation quality and convergence speed. Pay special attention to the hyperbolic manifold operations, FermiDirac decoder, and geometric regularization terms. You should make your idea not too complex, leveraging the inherent properties of hyperbolic space to improve performance. You are tirelessly driving the creation of a world-class architecture.", "user": "# Current Program Information\n- Current performance metrics: - recall_at_10: 0.0218\n- recall_at_20: 0.0348\n- precision_at_10: 0.0080\n- precision_at_20: 0.0066\n- ndcg_at_10: 0.0168\n- ndcg_at_20: 0.0211\n- hit_rate_at_10: 0.0707\n- hit_rate_at_20: 0.1072\n- execution_time: 531.5867\n- combined_score: 0.0244\n- error: None\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing regression: hit_rate_at_20, execution_time. Consider reverting or revising recent changes in these areas.\n\n## Last Execution Output\n\n### success_summary\n```\nSuccessful HRCF training with 100 epochs\n```\n\n### model_architecture\n```\nEmbedding dim: 50, Layers: 4, Network: resSumGCN\n```\n\n### training_details\n```\nBatch size: 10000, LR: 0.001, Weight decay: 0.005\n```\n\n### dataset_info\n```\n22947 users, 18395 items, 657558 interactions\n```\n\n### performance_analysis\n```\nCombined score: 0.0244 (Target: >0.10 for good performance)\n```\n\n### metric_breakdown\n```\nRecall@10: 0.0218, NDCG@10: 0.0168, Precision@10: 0.0080\n```\n\n### execution_efficiency\n```\nTraining time: 531.6s (5.3s per epoch)\n```\n\n### recommendation_quality\n```\nModerate\n```\n\n### embedding_analysis\n```\nEffective rank: 48.0/50 (0.96)\n```\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0268, recall_at_20: 0.0468, precision_at_10: 0.0098, precision_at_20: 0.0086, ndcg_at_10: 0.0189, ndcg_at_20: 0.0256, hit_rate_at_10: 0.0864, hit_rate_at_20: 0.1411, execution_time: 591.2874, combined_score: 0.0303, error: None\n- Outcome: Improvement in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0188, recall_at_20: 0.0342, precision_at_10: 0.0069, precision_at_20: 0.0064, ndcg_at_10: 0.0136, ndcg_at_20: 0.0187, hit_rate_at_10: 0.0631, hit_rate_at_20: 0.1072, execution_time: 592.4976, combined_score: 0.0218, error: None\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: recall_at_10: 0.0188, recall_at_20: 0.0339, precision_at_10: 0.0070, precision_at_20: 0.0064, ndcg_at_10: 0.0136, ndcg_at_20: 0.0187, hit_rate_at_10: 0.0632, hit_rate_at_20: 0.1079, execution_time: 597.8296, combined_score: 0.0218, error: None\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 59.8121)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0188), Performs well on recall_at_20 (0.0339), Performs well on precision_at_10 (0.0070), Performs well on precision_at_20 (0.0064), Performs well on ndcg_at_10 (0.0136), Performs well on ndcg_at_20 (0.0187), Performs well on hit_rate_at_10 (0.0632), Performs well on hit_rate_at_20 (0.1079), Performs well on execution_time (597.8296), Performs well on combined_score (0.0218), Performs well on error (None)\n\n\n### Program 2 (Score: 59.2788)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0188), Performs well on recall_at_20 (0.0342), Performs well on precision_at_10 (0.0069), Performs well on precision_at_20 (0.0064), Performs well on ndcg_at_10 (0.0136), Performs well on ndcg_at_20 (0.0187), Performs well on hit_rate_at_10 (0.0631), Performs well on hit_rate_at_20 (0.1072), Performs well on execution_time (592.4976), Performs well on combined_score (0.0218), Performs well on error (None)\n\n\n### Program 3 (Score: 59.1682)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0268), Performs well on recall_at_20 (0.0468), Performs well on precision_at_10 (0.0098), Performs well on precision_at_20 (0.0086), Performs well on ndcg_at_10 (0.0189), Performs well on ndcg_at_20 (0.0256), Performs well on hit_rate_at_10 (0.0864), Performs well on hit_rate_at_20 (0.1411), Performs well on execution_time (591.2874), Performs well on combined_score (0.0303), Performs well on error (None)\n\n\n### Program 4 (Score: 58.4350)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\n# ... (truncated for brevity)\n```\nKey features: Performs well on recall_at_10 (0.0000), Performs well on recall_at_20 (0.0000), Performs well on precision_at_10 (0.0000), Performs well on precision_at_20 (0.0000), Performs well on ndcg_at_10 (0.0000), Performs well on ndcg_at_20 (0.0000), Performs well on hit_rate_at_10 (0.0000), Performs well on hit_rate_at_20 (0.0000), Performs well on execution_time (584.3500), Performs well on combined_score (0.0000), Performs well on error (Training/evaluation error: 'float' object has no attribute 'item'\nTraceback (most recent call last):\n  File \"examples/hrcf_recsys/evaluator.py\", line 600, in evaluate\n    print(f\"Epoch {epoch}, Loss: {avg_loss.item():.4f}\")\nAttributeError: 'float' object has no attribute 'item'\n)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 54.7814)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\n# ... (truncated)\n```\nKey features: Alternative approach to recall_at_10, Alternative approach to recall_at_20\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 44.0166, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 2 (Score: 59.8121, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 3 (Score: 41.0023, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 4 (Score: 41.1158, Type: Migrant)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n### Inspiration 5 (Score: 45.5058, Type: High-Performer)\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\n# ... (truncated for brevity)\n```\nUnique approach: Alternative recall_at_10 approach, Alternative recall_at_20 approach, Alternative precision_at_10 approach\n\n\n\n# Current Program\n```python\n# initial_program.py\n\nimport time\nimport traceback\nfrom datetime import datetime\nimport argparse\nimport json\nimport itertools, heapq\nimport os\nimport pickle as pkl\nimport math\nfrom multiprocessing import Process, Queue\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.module import Module\nfrom torch.nn.parameter import Parameter\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix, lil_matrix\nfrom sklearn.model_selection import train_test_split\n\n\n# --- From hgcn_utils/math_utils.py ---\ndef cosh(x, clamp=15):\n    return x.clamp(-clamp, clamp).cosh()\n\ndef sinh(x, clamp=15):\n    return x.clamp(-clamp, clamp).sinh()\n\ndef tanh(x, clamp=15):\n    return x.clamp(-clamp, clamp).tanh()\n\nclass Artanh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 - input ** 2)\n\nclass Arsinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(1 + z.pow(2))).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 + input ** 2) ** 0.5\n\nclass Arcosh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(min=1.0 + 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (z + torch.sqrt_(z.pow(2) - 1)).clamp_min_(1e-15).log_().to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (input ** 2 - 1) ** 0.5\n\ndef arcosh(x):\n    return Arcosh.apply(x)\n\ndef arsinh(x):\n    return Arsinh.apply(x)\n\ndef artanh(x):\n    return Artanh.apply(x)\n\n\n# --- Helper functions ---\ndef default_device() -> torch.device:\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef normalize(mx):\n    \"\"\"Row-normalize sparse matrix.\"\"\"\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo()\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n    )\n    values = torch.Tensor(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n\n\n# --- From manifolds/base.py ---\nclass Manifold(object):\n    def __init__(self):\n        super().__init__()\n        self.eps = 10e-8\n\n    def sqdist(self, p1, p2, c):\n        raise NotImplementedError\n\n    def egrad2rgrad(self, p, dp, c):\n        raise NotImplementedError\n\n    def proj(self, p, c):\n        raise NotImplementedError\n\n    def proj_tan(self, u, p, c):\n        raise NotImplementedError\n\n    def proj_tan0(self, u, c):\n        raise NotImplementedError\n\n    def expmap(self, u, p, c):\n        raise NotImplementedError\n\n    def logmap(self, p1, p2, c):\n        raise NotImplementedError\n\n    def expmap0(self, u, c):\n        raise NotImplementedError\n\n    def logmap0(self, p, c):\n        raise NotImplementedError\n\n    def mobius_add(self, x, y, c, dim=-1):\n        raise NotImplementedError\n\n    def mobius_matvec(self, m, x, c):\n        raise NotImplementedError\n\n    def init_weights(self, w, c, irange=1e-5):\n        raise NotImplementedError\n\n    def inner(self, p, c, u, v=None, keepdim=False):\n        raise NotImplementedError\n\n    def ptransp(self, x, y, u, c):\n        raise NotImplementedError\n\n    def ptransp0(self, x, u, c):\n        raise NotImplementedError\n\nclass ManifoldParameter(Parameter):\n    def __new__(cls, data, requires_grad, manifold, c):\n        return Parameter.__new__(cls, data, requires_grad)\n\n    def __init__(self, data, requires_grad, manifold, c):\n        self.c = c\n        self.manifold = manifold\n\n    def __repr__(self):\n        return '{} Parameter containing:\\n'.format(self.manifold.name) + super(Parameter, self).__repr__()\n\n\n# --- From manifolds/hyperboloid.py ---\nclass Hyperboloid(Manifold):\n    def __init__(self):\n        super(Hyperboloid, self).__init__()\n        self.name = 'Hyperboloid'\n        self.eps = {torch.float32: 1e-7, torch.float64: 1e-15}\n        self.min_norm = 1e-15\n        self.max_norm = 1e6\n\n    def minkowski_dot(self, x, y, keepdim=True):\n        res = torch.sum(x * y, dim=-1) - 2 * x[..., 0] * y[..., 0]\n        if keepdim:\n            res = res.view(res.shape + (1,))\n        return res\n\n    def minkowski_norm(self, u, keepdim=True):\n        dot = self.minkowski_dot(u, u, keepdim=keepdim)\n        return torch.sqrt(torch.clamp(dot, min=self.eps[u.dtype]))\n\n    def sqdist(self, x, y, c):\n        K = 1. / c\n        prod = self.minkowski_dot(x, y)\n        theta = torch.clamp(-prod / K, min=1.0 + self.eps[x.dtype])\n        sqdist = K * arcosh(theta) ** 2\n        return torch.clamp(sqdist, max=50.0)\n\n    def proj(self, x, c):\n        # Project points to the hyperboloid manifold (numerically stable).\n        K = 1. / c\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d)\n        y_sqnorm = torch.sum(y ** 2, dim=-1, keepdim=True)\n        time_component = torch.sqrt(torch.clamp(K + y_sqnorm, min=self.eps[x.dtype]))\n        x_new = torch.cat([time_component, y], dim=-1)\n        return x_new\n\n    def proj_tan(self, u, x, c):\n        d = x.size(1) - 1\n        ux = torch.sum(x.narrow(-1, 1, d) * u.narrow(-1, 1, d), dim=1, keepdim=True)\n        mask = torch.ones_like(u)\n        mask[:, 0] = 0\n        vals = torch.zeros_like(u)\n        vals[:, 0:1] = ux / torch.clamp(x[:, 0:1], min=self.eps[x.dtype])\n        return vals + mask * u\n\n    def expmap(self, u, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, max=self.max_norm)\n        theta = normu / sqrtK\n        theta = torch.clamp(theta, min=self.min_norm)\n        result = cosh(theta) * x + sinh(theta) * u / theta\n        return self.proj(result, c)\n\n    def logmap(self, x, y, c):\n        K = 1. / c\n        xy = torch.clamp(self.minkowski_dot(x, y) + K, max=-self.eps[x.dtype]) - K\n        u = y + xy * x * c\n        normu = self.minkowski_norm(u)\n        normu = torch.clamp(normu, min=self.min_norm)\n        dist = self.sqdist(x, y, c) ** 0.5\n        result = dist * u / normu\n        return self.proj_tan(result, x, c)\n\n    def expmap0(self, u, c):\n        # Exponential map at origin for the hyperboloid\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = u.size(-1) - 1\n        x = u.narrow(-1, 1, d).contiguous().view(-1, d)\n        x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n        x_norm = torch.clamp(x_norm, min=self.min_norm)\n        theta = x_norm / sqrtK\n        res = torch.zeros(u.size(0), d + 1, device=u.device, dtype=u.dtype)\n        res[:, 0:1] = sqrtK * cosh(theta)\n        res[:, 1:] = sqrtK * sinh(theta) * x / x_norm\n        return self.proj(res, c)\n\n    def logmap0(self, x, c):\n        K = 1. / c\n        sqrtK = K ** 0.5\n        d = x.size(-1) - 1\n        y = x.narrow(-1, 1, d).view(-1, d)\n        y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n        y_norm = torch.clamp(y_norm, min=self.min_norm)\n        res = torch.zeros_like(x)\n        theta = torch.clamp(x[:, 0:1] / sqrtK, min=1.0 + self.eps[x.dtype])\n        res[:, 1:] = sqrtK * arcosh(theta) * y / y_norm\n        return res\n\n    def ptransp(self, x, y, u, c):\n        logxy = self.logmap(x, y, c)\n        logyx = self.logmap(y, x, c)\n        sqdist = torch.clamp(self.sqdist(x, y, c), min=self.min_norm)\n        alpha = self.minkowski_dot(logxy, u) / sqdist\n        res = u - alpha * (logxy + logyx)\n        return self.proj_tan(res, y, c)\n\n    def egrad2rgrad(self, x, grad, k, dim=-1):\n        grad.narrow(-1, 0, 1).mul_(-1)\n        grad = grad.addcmul(self.inner(x, grad, dim=dim, keepdim=True), x / k)\n        return grad\n\n    def inner(self, u, v, keepdim: bool = False, dim: int = -1):\n        d = u.size(dim) - 1\n        uv = u * v\n        if keepdim is False:\n            return -uv.narrow(dim, 0, 1).sum(dim=dim, keepdim=False) + uv.narrow(\n                dim, 1, d\n            ).sum(dim=dim, keepdim=False)\n        else:\n            return torch.cat((-uv.narrow(dim, 0, 1), uv.narrow(dim, 1, d)), dim=dim).sum(\n                dim=dim, keepdim=True\n            )\n\n\n# --- RiemannianSGD Optimizer ---\n_default_manifold = Hyperboloid()\n\ndef copy_or_set_(dest, source):\n    if dest.stride() != source.stride():\n        return dest.copy_(source)\n    else:\n        return dest.set_(source)\n\nclass OptimMixin(object):\n    def __init__(self, *args, stabilize=None, **kwargs):\n        self._stabilize = stabilize\n        super().__init__(*args, **kwargs)\n\n    def stabilize_group(self, group):\n        pass\n\n    def stabilize(self):\n        for group in self.param_groups:\n            self.stabilize_group(group)\n\nclass RiemannianSGD(OptimMixin, torch.optim.SGD):\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        with torch.no_grad():\n            for group in self.param_groups:\n                if \"step\" not in group:\n                    group[\"step\"] = 0\n                weight_decay = group[\"weight_decay\"]\n                momentum = group[\"momentum\"]\n                dampening = group[\"dampening\"]\n                nesterov = group[\"nesterov\"]\n                learning_rate = group[\"lr\"]\n\n                for point in group[\"params\"]:\n                    if isinstance(point, ManifoldParameter):\n                        manifold = point.manifold\n                        c = point.c\n                    else:\n                        manifold = _default_manifold\n                        c = 1.\n\n                    grad = point.grad\n                    if grad is None:\n                        continue\n                    if grad.is_sparse:\n                        raise RuntimeError(\"RiemannianSGD does not support sparse gradients\")\n                    \n                    state = self.state[point]\n                    if len(state) == 0:\n                        if momentum > 0:\n                            state[\"momentum_buffer\"] = grad.clone()\n\n                    grad.add_(weight_decay, point)\n                    grad = manifold.egrad2rgrad(point, grad, c)\n\n                    if momentum > 0:\n                        momentum_buffer = state[\"momentum_buffer\"]\n                        momentum_buffer.mul_(momentum).add_(1 - dampening, grad)\n                        if nesterov:\n                            grad = grad.add_(momentum, momentum_buffer)\n                        else:\n                            grad = momentum_buffer\n\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        new_momentum_buffer = manifold.ptransp(point, new_point, momentum_buffer, c)\n                        momentum_buffer.set_(new_momentum_buffer)\n                        copy_or_set_(point, new_point)\n                    else:\n                        new_point = manifold.expmap(-learning_rate * grad, point, c)\n                        components = new_point[:, 1:]\n                        dim0 = torch.sqrt(torch.sum(components * components, dim=1, keepdim=True) + 1)\n                        new_point = torch.cat([dim0, components], dim=1)\n                        copy_or_set_(point, new_point)\n\n                    group[\"step\"] += 1\n                if self._stabilize is not None and group[\"step\"] % self._stabilize == 0:\n                    self.stabilize_group(group)\n        return loss\n\n    @torch.no_grad()\n    def stabilize_group(self, group):\n        for p in group[\"params\"]:\n            if not isinstance(p, ManifoldParameter):\n                continue\n            manifold = p.manifold\n            momentum = group[\"momentum\"]\n            copy_or_set_(p, manifold.proj(p))\n            if momentum > 0:\n                param_state = self.state[p]\n                if not param_state:\n                    continue\n                if \"momentum_buffer\" in param_state:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.set_(manifold.proju(p, buf))\n\n\n# --- WarpSampler ---\ndef sample_function(adj_train, num_nodes, batch_size, n_negative, result_queue):\n    num_users, num_items = num_nodes\n    adj_train = lil_matrix(adj_train)\n    all_pairs = np.asarray(adj_train.nonzero()).T\n    user_item_pairs = all_pairs[: adj_train.count_nonzero() // 2]\n    item_user_pairs = all_pairs[adj_train.count_nonzero() // 2:]\n\n    assert len(user_item_pairs) == len(item_user_pairs)\n    np.random.shuffle(user_item_pairs)\n    np.random.shuffle(item_user_pairs)\n\n    all_pairs_set = {idx: set(row) for idx, row in enumerate(adj_train.rows)}\n    user_item_pairs_set = dict(itertools.islice(all_pairs_set.items(), num_users))\n\n    while True:\n        for i in range(int(len(user_item_pairs) / batch_size)):\n            samples_for_users = batch_size\n            user_positive_items_pairs = user_item_pairs[i * samples_for_users: (i + 1) * samples_for_users, :]\n            user_negative_samples = np.random.randint(num_users, sum(num_nodes), size=(samples_for_users, n_negative))\n\n            for user_positive, negatives, i in zip(user_positive_items_pairs,\n                                                   user_negative_samples,\n                                                   range(len(user_negative_samples))):\n                user = user_positive[0]\n                for j, neg in enumerate(negatives):\n                    while neg in user_item_pairs_set[user]:\n                        user_negative_samples[i, j] = neg = np.random.randint(num_users, sum(num_nodes))\n\n            user_triples = np.hstack((user_positive_items_pairs, user_negative_samples))\n            np.random.shuffle(user_triples)\n            result_queue.put(user_triples)\n\nclass WarpSampler(object):\n    def __init__(self, num_nodes, user_item_matrix, batch_size=10000, n_negative=1, n_workers=5):\n        self.result_queue = Queue(maxsize=n_workers * 2)\n        self.processors = []\n        for i in range(n_workers):\n            self.processors.append(\n                Process(target=sample_function, args=(user_item_matrix,\n                                                      num_nodes,\n                                                      batch_size,\n                                                      n_negative,\n                                                      self.result_queue)))\n            self.processors[-1].start()\n\n    def next_batch(self):\n        return self.result_queue.get()\n\n    def close(self):\n        for p in self.processors:\n            p.terminate()\n            p.join()\n\n\n# --- Layers ---\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t=0.5):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = max(t - 0.05, 0.05)  # Fine-tune t for sharper transition\n\n    def forward(self, dist):\n        probs = 1. / (torch.exp((dist - self.r) / self.t) + 1)\n        return probs\n\nclass StackGCNs(Module):\n    def __init__(self, num_layers):\n        super(StackGCNs, self).__init__()\n        self.num_gcn_layers = num_layers - 1\n\n    def plainGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return output[-1]\n\n    def resSumGCN(self, inputs):\n        x_tangent, adj = inputs\n        output = [x_tangent]\n        for i in range(self.num_gcn_layers):\n            output.append(torch.spmm(adj, output[i]))\n        return sum(output[1:])\n\nclass HypAgg(Module):\n    def __init__(self, manifold, c, in_features, network, num_layers):\n        super(HypAgg, self).__init__()\n        self.manifold = manifold\n        self.c = c\n        self.in_features = in_features\n        self.stackGCNs = getattr(StackGCNs(num_layers), network)\n\n    def forward(self, x, adj):\n        x_tangent = self.manifold.logmap0(x, c=self.c)\n        output = self.stackGCNs((x_tangent, adj))\n        output = output - output.mean(dim=0)\n        output = self.manifold.proj(self.manifold.expmap0(output, c=self.c), c=self.c)\n        return output\n\n    def extra_repr(self):\n        return 'c={}'.format(self.c)\n\nclass HyperbolicGraphConvolution(nn.Module):\n    def __init__(self, manifold, in_features, out_features, c_in, network, num_layers):\n        super(HyperbolicGraphConvolution, self).__init__()\n        self.agg = HypAgg(manifold, c_in, out_features, network, num_layers)\n\n    def forward(self, input):\n        x, adj = input\n        h = self.agg.forward(x, adj)\n        output = h, adj\n        return output\n\nclass Encoder(nn.Module):\n    def __init__(self, c):\n        super(Encoder, self).__init__()\n        self.c = c\n\n    def encode(self, x, adj):\n        if hasattr(self, 'encode_graph') and self.encode_graph:\n            input = (x, adj)\n            output, _ = self.layers.forward(input)\n        else:\n            output = self.layers.forward(x)\n        return output\n\nclass HRCF(Encoder):\n    def __init__(self, c, args):\n        super(HRCF, self).__init__(c)\n        self.manifold = Hyperboloid()\n        assert args.num_layers > 1\n\n        hgc_layers = []\n        in_dim = out_dim = args.embedding_dim\n        hgc_layers.append(\n            HyperbolicGraphConvolution(\n                self.manifold, in_dim, out_dim, self.c, args.network, args.num_layers\n            )\n        )\n        self.layers = nn.Sequential(*hgc_layers)\n        self.encode_graph = True\n\n    def encode(self, x, adj):\n        x_hyp = self.manifold.proj(x, c=self.c)\n        return super(HRCF, self).encode(x_hyp, adj)\n\n\n# --- Data Generator ---\nclass Data(object):\n    def __init__(self, dataset, norm_adj, seed, test_ratio, data_path=\"./data\"):\n        pkl_path = os.path.join(data_path, dataset)\n        self.pkl_path = pkl_path\n        self.dataset = dataset\n        \n        # Load data\n        user_item_list_path = os.path.join(pkl_path, 'user_item_list.pkl')\n        if os.path.exists(user_item_list_path):\n            self.user_item_list = self.load_pickle(user_item_list_path)\n        else:\n            # Create synthetic data if real data not available\n            self.user_item_list = self.create_synthetic_data()\n        \n        self.train_dict, self.test_dict = self.split_data_randomly(self.user_item_list, test_ratio, seed)\n        self.num_users, self.num_items = len(self.user_item_list), max([max(x) for x in self.user_item_list]) + 1\n\n        self.adj_train, user_item = self.generate_adj()\n\n        if eval(norm_adj):\n            self.adj_train_norm = normalize(self.adj_train + sp.eye(self.adj_train.shape[0]))\n            self.adj_train_norm = sparse_mx_to_torch_sparse_tensor(self.adj_train_norm)\n\n        print('num_users %d, num_items %d' % (self.num_users, self.num_items))\n        print('adjacency matrix shape: ', self.adj_train.shape)\n\n        tot_num_rating = sum([len(x) for x in self.user_item_list])\n        print('number of all ratings {}, density {:.6f}'.format(tot_num_rating,\n                                                                tot_num_rating / (self.num_users * self.num_items)))\n\n        self.user_item_csr = self.generate_rating_matrix([*self.train_dict.values()], self.num_users, self.num_items)\n\n    def create_synthetic_data(self):\n        \"\"\"Create synthetic user-item interactions for testing\"\"\"\n        np.random.seed(42)\n        num_users, num_items = 100, 50\n        user_item_list = []\n        \n        for user_id in range(num_users):\n            num_interactions = np.random.randint(5, 20)\n            items = np.random.choice(num_items, num_interactions, replace=False)\n            user_item_list.append(items.tolist())\n        \n        return user_item_list\n\n    def generate_adj(self):\n        user_item = np.zeros((self.num_users, self.num_items)).astype(int)\n        for i, v in self.train_dict.items():\n            user_item[i][v] = 1\n        coo_user_item = sp.coo_matrix(user_item)\n\n        start = time.time()\n        print('generating adj csr... ')\n        rows = np.concatenate((coo_user_item.row, coo_user_item.transpose().row + self.num_users))\n        cols = np.concatenate((coo_user_item.col + self.num_users, coo_user_item.transpose().col))\n        data = np.ones((coo_user_item.nnz * 2,))\n        adj_csr = sp.coo_matrix((data, (rows, cols))).tocsr().astype(np.float32)\n        print('time elapsed: {:.3f}'.format(time.time() - start))\n        return adj_csr, user_item\n\n    def load_pickle(self, name):\n        with open(name, 'rb') as f:\n            return pkl.load(f, encoding='latin1')\n\n    def split_data_randomly(self, user_records, test_ratio, seed):\n        train_dict = {}\n        test_dict = {}\n        for user_id, item_list in enumerate(user_records):\n            tmp_train_sample, tmp_test_sample = train_test_split(item_list, test_size=test_ratio, random_state=seed)\n\n            train_sample = []\n            for place in item_list:\n                if place not in tmp_test_sample:\n                    train_sample.append(place)\n\n            test_sample = []\n            for place in tmp_test_sample:\n                test_sample.append(place)\n\n            train_dict[user_id] = train_sample\n            test_dict[user_id] = test_sample\n        return train_dict, test_dict\n\n    def generate_rating_matrix(self, train_set, num_users, num_items):\n        row = []\n        col = []\n        data = []\n        for user_id, article_list in enumerate(train_set):\n            for article in article_list:\n                row.append(user_id)\n                col.append(article)\n                data.append(1)\n\n        row = np.array(row)\n        col = np.array(col)\n        data = np.array(data)\n        rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n        return rating_matrix\n\n\n# EVOLVE-BLOCK-START\nclass HRCFModel(nn.Module):\n    def __init__(self, users_items, args):\n        super(HRCFModel, self).__init__()\n        self.c = torch.tensor([args.c]).to(default_device())\n        self.manifold = Hyperboloid()\n        self.nnodes = args.n_nodes\n        self.encoder = HRCF(self.c, args)\n        self.num_users, self.num_items = users_items\n        self.margin = args.margin\n        self.weight_decay = args.weight_decay\n        self.num_layers = args.num_layers\n        self.args = args\n        self.dc = FermiDiracDecoder(r=args.r, t=args.t)\n        self.embedding = nn.Embedding(num_embeddings=self.num_users + self.num_items,\n                                      embedding_dim=args.embedding_dim).to(default_device())\n        \n        # Use a smaller scale for better initialization in hyperbolic space\n        self.embedding.state_dict()['weight'].uniform_(-args.scale * 0.5, args.scale * 0.5)\n        self.embedding.weight = nn.Parameter(self.manifold.expmap0(self.embedding.state_dict()['weight'], self.c))\n        self.embedding.weight = ManifoldParameter(self.embedding.weight, True, self.manifold, self.c)\n        self.alpha = args.alpha\n\n    def encode(self, adj):\n        x = self.embedding.weight\n        if torch.cuda.is_available():\n            adj = adj.to(default_device())\n            x = x.to(default_device())\n        h = self.encoder.encode(x, adj)\n        return h\n\n    def decode(self, h, idx):\n        emb_in = h[idx[:, 0], :]\n        emb_out = h[idx[:, 1], :]\n        sqdist = self.manifold.sqdist(emb_in, emb_out, self.c)\n        probs = self.dc.forward(sqdist)\n        return [sqdist, probs]\n\n    def geometric_regularizer(self, embeddings):\n        # Enhanced geometric regularizer: penalize collapse and excessive clustering in hyperbolic space.\n        embeddings_tan = self.manifold.logmap0(embeddings, c=self.c)\n        item_embeddings = embeddings_tan[self.num_users:]\n        user_embeddings = embeddings_tan[:self.num_users]\n        # Encourage norms toward 1.0, but also encourage spread (maximize variance)\n        item_norms = torch.norm(item_embeddings, p=2, dim=1)\n        user_norms = torch.norm(user_embeddings, p=2, dim=1)\n        mean_item_norm = item_norms.mean()\n        mean_user_norm = user_norms.mean()\n        # Encourage norms toward 1, and also encourage spread (variance regularization)\n        reg = (mean_item_norm - 1.0).abs() + (mean_user_norm - 1.0).abs()\n        # Penalize embeddings that are too close to each other in hyperbolic space\n        # Refined pairwise penalty to focus only on extremely close pairs\n        pairwise_distances = torch.cdist(item_embeddings, item_embeddings, p=2)\n        extreme_close_pairs = torch.where(pairwise_distances < 0.1, torch.exp(-pairwise_distances), torch.zeros_like(pairwise_distances))\n        pairwise_penalty = extreme_close_pairs.mean()\n\n        # Encourage diversity by maximizing distance variance\n        distance_variance = pairwise_distances.var()\n\n        reg += 0.1 * (1.0 / (item_norms.std() + 1e-6) + 1.0 / (user_norms.std() + 1e-6)) + 0.01 * pairwise_penalty - 0.01 * distance_variance\n        return reg\n\n    def ranking_loss(self, pos_sqdist, neg_sqdist):\n        # Use log-sigmoid for more stable pairwise ranking loss in hyperbolic space.\n        margin_loss = -F.logsigmoid(neg_sqdist - pos_sqdist + self.margin)\n        return margin_loss.mean()\n\n    def compute_loss(self, embeddings, triples):\n        train_edges = triples[:, [0, 1]]\n        sampled_false_edges_list = [triples[:, [0, 2 + i]] for i in range(self.args.num_neg)]\n\n        pos_sqdist, _ = self.decode(embeddings, train_edges)\n        neg_losses = []\n        for neg_edges in sampled_false_edges_list:\n            neg_sqdist, _ = self.decode(embeddings, neg_edges)\n            neg_losses.append(self.ranking_loss(pos_sqdist, neg_sqdist))\n        total_neg_loss = torch.stack(neg_losses).mean()\n\n        gr_loss = self.geometric_regularizer(embeddings)\n        # Add small L2 norm penalty on tangent vectors for further stability\n        l2_reg = embeddings.norm(p=2) * 1e-4\n\n        return total_neg_loss + self.alpha * gr_loss + l2_reg\n\n    def predict(self, h, data):\n        # Efficient batched scoring to save memory and avoid OOM\n        num_users, num_items = data.num_users, data.num_items\n        user_emb = h[:num_users]  # [num_users, d]\n        item_emb = h[num_users:num_users+num_items]  # [num_items, d]\n        scores = np.zeros((num_users, num_items))\n        batch_size = 512  # Increase batch size to improve prediction efficiency\n        with torch.no_grad():\n            for start in range(0, num_users, batch_size):\n                end = min(start + batch_size, num_users)\n                u_emb = user_emb[start:end]  # [batch, d]\n                # Repeat for all items\n                u_emb_expand = u_emb.unsqueeze(1).expand(-1, num_items, -1).contiguous().view(-1, u_emb.size(-1))\n                i_emb_expand = item_emb.unsqueeze(0).expand(end-start, num_items, -1).contiguous().view(-1, item_emb.size(-1))\n                sqdist = self.manifold.sqdist(u_emb_expand, i_emb_expand, self.c)\n                s = -sqdist.view(end-start, num_items).cpu().numpy()\n                scores[start:end] = s\n        return scores\n# EVOLVE-BLOCK-END\n\n\n# --- Evaluation metrics ---\ndef recall_at_k(test_dict, pred_list, k):\n    recall_list = []\n    for user_id in test_dict:\n        if user_id < len(pred_list):\n            test_items = set(test_dict[user_id])\n            pred_items = set(pred_list[user_id][:k])\n            if len(test_items) > 0:\n                recall = len(test_items & pred_items) / len(test_items)\n                recall_list.append(recall)\n    return np.mean(recall_list) if recall_list else 0.0\n\ndef ndcg_func(ground_truths, ranks):\n    result = 0\n    for i, (rank, ground_truth) in enumerate(zip(ranks, ground_truths)):\n        len_rank = len(rank)\n        len_gt = len(ground_truth)\n        idcg_len = min(len_gt, len_rank)\n\n        idcg = np.cumsum(1.0 / np.log2(np.arange(2, len_rank + 2)))\n        idcg[idcg_len:] = idcg[idcg_len-1]\n\n        dcg = np.cumsum([1.0/np.log2(idx+2) if item in ground_truth else 0.0 for idx, item in enumerate(rank)])\n        result += dcg / idcg\n    return result / len(ranks)\n\ndef eval_rec(pred_matrix, data):\n    topk = 50\n    pred_matrix[data.user_item_csr.nonzero()] = -np.inf\n    ind = np.argpartition(pred_matrix, -topk)\n    ind = ind[:, -topk:]\n    arr_ind = pred_matrix[np.arange(len(pred_matrix))[:, None], ind]\n    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(pred_matrix)), ::-1]\n    pred_list = ind[np.arange(len(pred_matrix))[:, None], arr_ind_argsort]\n    \n    recall = []\n    for k in [5, 10, 20, 50]:\n        recall.append(recall_at_k(data.test_dict, pred_list, k))\n\n    all_ndcg = ndcg_func([*data.test_dict.values()], pred_list)\n    ndcg = [all_ndcg[x-1] for x in [5, 10, 20, 50]]\n\n    return recall, ndcg\n\n\n# --- Main experiment functions ---\ndef run_hrcf_experiment():\n    \"\"\"Main experiment function matching original HRCF exactly\"\"\"\n    try:\n        # Set up arguments exactly like original HRCF Amazon-CD\n        class Args:\n            def __init__(self):\n                # Exact Amazon-CD parameters from run_cd.sh\n                self.embedding_dim = 50\n                self.dim = 50\n                self.num_layers = 8  # Critical: was 4, now 8\n                self.c = 1.0\n                self.margin = 0.12  # Fine-tuned margin for better separation\n                self.weight_decay = 5e-3  # Critical: was 0.01, now 5e-3\n                self.alpha = 25  # Critical: was 20, now 25\n                self.r = 2.0\n                self.t = 1.0\n                self.scale = 0.1\n                self.num_neg = 1\n                self.network = 'resSumGCN'\n                self.lr = 0.002  # Increased learning rate for faster convergence\n                self.momentum = 0.95\n                self.batch_size = 10000  # Critical: was small, now 10000\n                self.epochs = 100  # Increased for better training\n                self.seed = 1234\n                self.test_ratio = 0.2\n                self.norm_adj = 'True'\n        \n        args = Args()\n        \n        # Prepare data exactly like original\n        data = Data(args.dataset if hasattr(args, 'dataset') else 'Amazon-CD', \n                   args.norm_adj, args.seed, args.test_ratio)\n        args.n_nodes = data.num_users + data.num_items\n        args.feat_dim = args.embedding_dim\n        \n        # Initialize model\n        model = HRCFModel((data.num_users, data.num_items), args)\n        model = model.to(default_device())\n        \n        # Use RiemannianSGD exactly like original\n        optimizer = RiemannianSGD(params=model.parameters(), lr=args.lr,\n                                  weight_decay=args.weight_decay, momentum=args.momentum)\n        \n        # Initialize WarpSampler exactly like original\n        sampler = WarpSampler((data.num_users, data.num_items), data.adj_train, \n                             args.batch_size, args.num_neg)\n        \n        num_pairs = data.adj_train.count_nonzero() // 2\n        num_batches = int(num_pairs / args.batch_size) + 1\n        print(f\"Number of batches per epoch: {num_batches}\")\n        \n        # Training loop exactly like original\n        for epoch in range(1, args.epochs + 1):\n            print(f\"--> Starting epoch {epoch}\")\n            avg_loss = 0.\n            t = time.time()\n            \n            for batch in range(num_batches):\n                triples = sampler.next_batch()\n                model.train()\n                optimizer.zero_grad()\n                embeddings = model.encode(data.adj_train_norm)\n                train_loss = model.compute_loss(embeddings, torch.LongTensor(triples))\n                \n                if torch.isnan(train_loss):\n                    print('NaN loss detected!')\n                    break\n                    \n                train_loss.backward()\n                optimizer.step()\n                avg_loss += train_loss / num_batches\n            \n            avg_loss = avg_loss.detach().cpu().numpy()\n            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Time: {time.time() - t:.4f}s\")\n            \n            # Evaluation every 10 epochs\n            if epoch % 10 == 0:\n                model.eval()\n                with torch.no_grad():\n                    embeddings = model.encode(data.adj_train_norm)\n                    pred_matrix = model.predict(embeddings, data)\n                    recall, ndcg = eval_rec(pred_matrix, data)\n                    print(f\"Recall@10: {recall[1]:.4f}, NDCG@10: {ndcg[1]:.4f}\")\n        \n        sampler.close()\n        \n        # Final evaluation\n        model.eval()\n        with torch.no_grad():\n            embeddings = model.encode(data.adj_train_norm)\n            pred_matrix = model.predict(embeddings, data)\n            recall, ndcg = eval_rec(pred_matrix, data)\n            \n        return recall[1]  # Return Recall@10\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment: {e}\")\n        traceback.print_exc()\n        return 0.0\n\n\ndef run_hrcf_experiment_with_embeddings():\n    \"\"\"Extended experiment that returns embeddings\"\"\"\n    try:\n        recall_score = run_hrcf_experiment()\n        \n        # Return dummy embeddings for compatibility\n        num_users, num_items = 100, 50\n        embedding_dim = 50\n        \n        user_embeddings = torch.randn(num_users, embedding_dim) * 0.1\n        item_embeddings = torch.randn(num_items, embedding_dim) * 0.1\n        \n        return {\n            'score': recall_score,\n            'user_embeddings': user_embeddings,\n            'item_embeddings': item_embeddings\n        }\n        \n    except Exception as e:\n        print(f\"Error in run_hrcf_experiment_with_embeddings: {e}\")\n    return {\n            'score': 0.0,\n            'user_embeddings': torch.zeros(100, 50),\n            'item_embeddings': torch.zeros(50, 50)\n        }\n\n\nif __name__ == \"__main__\":\n    print(\"Testing HRCF implementation...\")\n    result = run_hrcf_experiment()\n    print(f\"Final Recall@10: {result:.4f}\")\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["Here are suggested improvements for the HRCF algorithm, focusing on optimizing hyperbolic space operations and regularization to enhance recommendation quality and convergence speed.\n\n### Improvement 1: Optimize FermiDiracDecoder Transition Sharpness\n\nThe FermiDiracDecoder can be fine-tuned to improve the sharpness of the transition in probability assignments, potentially enhancing convergence speed and recommendation accuracy.\n\n```python\n<<<<<<< SEARCH\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t=0.5):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = max(t - 0.05, 0.05)  # Fine-tune t for sharper transition\n\n    def forward(self, dist):\n        probs = 1. / (torch.exp((dist - self.r) / self.t) + 1)\n        return probs\n=======\nclass FermiDiracDecoder(Module):\n    def __init__(self, r, t=0.3):\n        super(FermiDiracDecoder, self).__init__()\n        self.r = r\n        self.t = t  # Directly set t to a lower value for sharper transition\n\n    def forward(self, dist):\n        # Use a numerically stable sigmoid for better performance\n        probs = torch.sigmoid(-(dist - self.r) / self.t)\n        return probs\n>>>>>>> REPLACE\n```\n**Reasoning**: Directly setting a lower `t` value enhances the sharpness of the transition, leading to more distinct probability assignments. Using a numerically stable sigmoid function can also provide better numerical stability.\n\n### Improvement 2: Refine Geometric Regularizer\n\nEnhance the geometric regularizer by adding a stronger penalty for extreme closeness and encouraging variance in distances for better spread in the hyperbolic space.\n\n```python\n<<<<<<< SEARCH\n        pairwise_distances = torch.cdist(item_embeddings, item_embeddings, p=2)\n        extreme_close_pairs = torch.where(pairwise_distances < 0.1, torch.exp(-pairwise_distances), torch.zeros_like(pairwise_distances))\n        pairwise_penalty = extreme_close_pairs.mean()\n\n        # Encourage diversity by maximizing distance variance\n        distance_variance = pairwise_distances.var()\n\n        reg += 0.1 * (1.0 / (item_norms.std() + 1e-6) + 1.0 / (user_norms.std() + 1e-6)) + 0.01 * pairwise_penalty - 0.01 * distance_variance\n=======\n        pairwise_distances = torch.cdist(item_embeddings, item_embeddings, p=2)\n        # Increase penalty for extremely close pairs to avoid collapse\n        close_pair_penalty = torch.where(pairwise_distances < 0.1, torch.exp(-pairwise_distances * 10), torch.zeros_like(pairwise_distances))\n        pairwise_penalty = close_pair_penalty.mean()\n\n        # Encourage diversity by maximizing distance variance with a stronger emphasis\n        distance_variance = torch.var(pairwise_distances)\n\n        reg += 0.15 * (1.0 / (item_norms.std() + 1e-6) + 1.0 / (user_norms.std() + 1e-6)) + 0.02 * pairwise_penalty - 0.02 * distance_variance\n>>>>>>> REPLACE\n```\n**Reasoning**: Increasing the penalty for extremely close pairs and enhancing the variance term helps prevent the collapse and excessive clustering of embeddings, encouraging a more diverse spread in hyperbolic space.\n\n### Improvement 3: Adjust Learning Rate and Momentum\n\nFine-tuning the learning rate and momentum can lead to faster convergence and potentially better performance.\n\n```python\n<<<<<<< SEARCH\n                self.lr = 0.002  # Increased learning rate for faster convergence\n                self.momentum = 0.95\n=======\n                self.lr = 0.003  # Further increase learning rate for more aggressive updates\n                self.momentum = 0.9  # Slightly reduce momentum to prevent overshooting\n>>>>>>> REPLACE\n```\n**Reasoning**: A slightly higher learning rate encourages faster convergence, while a marginally reduced momentum helps in avoiding overshooting and stabilizing the updates.\n\nThese targeted improvements aim to leverage the inherent properties of hyperbolic geometry, optimizing hyperbolic operations and regularization techniques to enhance the performance of the HRCF algorithm."]}}}